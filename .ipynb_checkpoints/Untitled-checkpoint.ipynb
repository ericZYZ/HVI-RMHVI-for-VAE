{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'hmc_base_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ed19f1029b18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhmc_base_pytorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhmc_unconstrained_pytorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'hmc_base_pytorch'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from hmc_base_pytorch import *\n",
    "from hmc_unconstrained_pytorch import *\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.normal import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "cuda = True\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "z_dim = 20\n",
    "\n",
    "# Data preparation\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "train_data = datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.MNIST('../data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "def binarization(data):\n",
    "    s = np.random.uniform(size = data.shape)\n",
    "    out = np.array(s<data).astype(float)\n",
    "    return out\n",
    "\n",
    "result = []\n",
    "for batch_idx, (data, _) in enumerate(train_loader):\n",
    "    data = data.view(-1, 784).numpy()\n",
    "    bi_data = binarization(data)\n",
    "    d = torch.from_numpy(bi_data)\n",
    "    result.append(d)\n",
    "    \n",
    "result_test = []\n",
    "for batch_idx, (data, _) in enumerate(test_loader):\n",
    "    data = data.view(-1, 784).numpy()\n",
    "    bi_data = binarization(data)\n",
    "    d = torch.from_numpy(bi_data)\n",
    "    result_test.append(d)\n",
    "\n",
    "############################################################3\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5*logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "def log_prior(z):\n",
    "    dim = z.shape[1]\n",
    "    mean = torch.zeros(dim).cuda()\n",
    "    cov = torch.eye(dim).cuda()\n",
    "    m = MultivariateNormal(mean, cov)\n",
    "    m.requires_grad=True\n",
    "    return m.log_prob(z)\n",
    "\n",
    "def multivariate_normal_logpdf(mean, cov, x):\n",
    "    mean = mean.cuda()\n",
    "    cov = cov.cuda()\n",
    "    k = x.shape[0]\n",
    "    t1 = -0.5*(x - mean).view(1, k)@torch.inverse(cov)@(x - mean).view(k, 1)\n",
    "    t2 = 0.5*k*torch.log(2*torch.tensor([math.pi]).cuda()) + 0.5*torch.log(torch.det(cov))\n",
    "    return t1 - t2\n",
    "\n",
    "def multivariate_normal_diagonal_logpdf(mean, cov_diag, x):\n",
    "    mean = mean.cuda()\n",
    "    cov_diag = cov_diag.cuda()\n",
    "    n = x.shape[0] # number of samples\n",
    "    k = x.shape[1] # dimension\n",
    "    t1 = -0.5*(x - mean)*(1/cov_diag)*(x-mean)\n",
    "    t1 = torch.sum(t1, dim=1)\n",
    "    #t2 = 0.5*k*torch.log(2*torch.tensor([math.pi]).cuda()) + 0.5*torch.log(torch.prod(cov_diag,1)).cuda()\n",
    "    t2 = 0.5*k*torch.log(2*torch.tensor([math.pi]).cuda()) + 0.5*torch.sum(torch.log(cov_diag)).cuda()\n",
    "    #print(\"t1: \"+str(t1)+\"t2: \"+str(t2))\n",
    "    return t1 - t2\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, 784)\n",
    "    # single hidden layer\n",
    "    def forward(self, x):\n",
    "        #x = x.view(-1, 784)\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return F.sigmoid(self.fc2(h1))\n",
    "    \n",
    "class q_z0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(q_z0, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 300)\n",
    "        #self.fc2 = nn.Linear(300, 300)\n",
    "        self.fc31 = nn.Linear(300, z_dim)\n",
    "        self.fc32 = nn.Linear(300, z_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        h1 = F.tanh(self.fc1(x))\n",
    "        #h2 = F.softplus(self.fc2(h1))\n",
    "        logvar = self.fc31(h1)\n",
    "        mu = self.fc32(h1)\n",
    "        return mu, logvar\n",
    "    \n",
    "    \n",
    "decoder = decoder().to(device)\n",
    "q_z0 = q_z0().to(device)\n",
    "r_v = r_v().to(device)\n",
    "q_v = q_v().to(device)\n",
    "#mass_diag = torch.randn(z_dim, requires_grad=True)\n",
    "log_mass_diag = torch.randn(z_dim, requires_grad=True)\n",
    "q_z0_mean = torch.randn(z_dim, requires_grad=True) \n",
    "q_z0_logvar = torch.randn(z_dim, requires_grad=True)\n",
    "\n",
    "\n",
    "def lower_bound(decoder, q_z0, r_v, data, T):\n",
    "    batch_size = data.view(-1, 784).shape[0]\n",
    "    data = data.to(device)\n",
    "    \n",
    "    \n",
    "    mu_z0, logvar_z0 = q_z0(data)\n",
    "    var_z0 = torch.exp(logvar_z0)\n",
    "    \n",
    "    \n",
    "    z0 = reparameterize(mu_z0, logvar_z0)\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get joint probaility p(x, z0)\n",
    "    log_prior_z0 = log_prior(z0)\n",
    "    #print(\"log_prior_z0: \" + str(log_prior_z0.shape))\n",
    "    decoder_output = decoder(z0)\n",
    "    \n",
    "    log_likelihood = 0. - F.binary_cross_entropy(decoder_output, data.view(-1, 784).float(), size_average=False, reduce=False)\n",
    "    #print(\"log_likelihood: \" + str(log_likelihood.shape))\n",
    "    log_likelihood = torch.sum(log_likelihood, dim = 1)\n",
    "    #print(\"log_likelihood: \" + str(log_likelihood.shape))\n",
    "    log_joint = log_prior_z0 + log_likelihood\n",
    "    #print(\"log_joint: \" + str(log_joint.shape))\n",
    "    \"\"\"\n",
    "    # get log q_z0\n",
    "    log_q_z0 = multivariate_normal_diagonal_logpdf(mu_z0, var_z0, z0)\n",
    "    \n",
    "    L = log_prior_z0 - log_q_z0\n",
    "    \n",
    "    print(\"initial L \"+str(L))\n",
    "    #print(L.shape)\n",
    "\n",
    "    #print(\"====================================\")\n",
    "    for i in range(T):\n",
    "       \n",
    "            one_log_alpha = torch.tensor([0.]).cuda()\n",
    "            \"\"\"\n",
    "            print(\"~~~~~~~~~~~`\")\n",
    "            print(log_joint_t[j])\n",
    "            print(log_r_vt[j])\n",
    "            print(log_joint[j])\n",
    "            print(log_q_v1[j])\n",
    "            \"\"\"  \n",
    "            \n",
    "            L[j] = L[j] + one_log_alpha\n",
    "            #print(\"L: \"+str(L))\n",
    "            #alpha = alpha + one_alpha\n",
    "        #L = L + torch.log(alpha)\n",
    "    #print(\"final L: \"+str(L.shape))\n",
    "    #print(\"~~~~~~~~~~~~~~~~~~~ new L \" + str(L) + \" ~~~~~~~~~~~~~~~~~~~\")\n",
    "    return torch.sum(L)/batch_size    \n",
    "                    \n",
    "                \n",
    "    \n",
    "# Train\n",
    "params1 = list(decoder.parameters())+list(r_v.parameters())#+list(q_z0.parameters())\n",
    "optimizer1 = optim.Adam(params1, lr=0.0005, weight_decay=5e-5)\n",
    "#optimizer2 = optim.Adam([log_mass_diag], lr=0.0005, weight_decay=1e-4)\n",
    "optimizer2 = optim.Adam(q_z0.parameters(), lr=0.00001, weight_decay=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(\"Epoch: \"+str(epoch+1))\n",
    "    file = open(\"result5_\"+str(epoch)+\".txt\",\"w\")\n",
    "    file_test = open(\"result5_test_\"+str(epoch)+\".txt\",\"w\")\n",
    "    for i in range(len(result)):\n",
    "        print(\"++++++++++ batch: \" + str(i) + \" ++++++++++\")\n",
    "\n",
    "        data = result[i].float()\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        #optimizer3.zero_grad()\n",
    "        L = lower_bound(decoder, q_z0, r_v, data, 1)\n",
    "        loss = 0. - L\n",
    "        loss.backward()\n",
    "        \n",
    "        #nn.utils.clip_grad_norm_(q_v.parameters(), 0.5)\n",
    "        #nn.utils.clip_grad_norm_(q_z0.parameters(), 1)\n",
    "        #nn.utils.clip_grad_norm_(decoder.parameters(), 1)\n",
    "        #nn.utils.clip_grad_norm_(r_v.parameters(), 1)\n",
    "        \n",
    "        print('weight grad after backward')\n",
    "        #print(net.conv1.bias.grad)\n",
    "        #print(q_z0.fc1.bias.grad)\n",
    "        #print(q_z0.fc31.bias.grad)\n",
    "        #print(q_z0.fc32.bias.grad)\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        #optimizer3.step()\n",
    "        file.write(str(0.-L.item())+\"\\n\") \n",
    "        print(L.item())\n",
    "    file.close()\n",
    "    for i in range(len(result_test)):\n",
    "        print(\"++++++++++ test batch: \" + str(i) + \" ++++++++++\")\n",
    "        data = result_test[i].float()\n",
    "        L = lower_bound(decoder, q_z0, r_v, data, 1)\n",
    "        file_test.write(str(0.-L.item())+\"\\n\")\n",
    "        print(L.item())\n",
    "    file_test.close()\n",
    "    \n",
    "    sample = torch.randn(64, 20).to(device)\n",
    "    sample = decoder(sample).cpu()\n",
    "    save_image(sample.view(64, 1, 28, 28), 'sample5_' + str(epoch) + '.png')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
