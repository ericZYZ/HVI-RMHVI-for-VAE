{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('F:\\Cambridge\\Project\\MHMC-for-VAE\\change_of_variable')\n",
    "sys.path.append('F:\\Cambridge\\Project\\MHMC-for-VAE\\hmc_pytorch')\n",
    "from change_of_variable_pytorch import * \n",
    "from hmc_base_pytorch import *\n",
    "from hmc_unconstrained_pytorch import *\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.normal import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.141592653589793"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "z_dim = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5*logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(z):\n",
    "    dim = z.shape[1]\n",
    "    mean = torch.zeros(dim).cuda()\n",
    "    cov = torch.eye(dim).cuda()\n",
    "    m = MultivariateNormal(mean, cov)\n",
    "    m.requires_grad=True\n",
    "    return m.log_prob(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_normal_logpdf(mean, cov, x):\n",
    "    mean = mean.cuda()\n",
    "    cov = cov.cuda()\n",
    "    k = x.shape[0]\n",
    "    #cov = cov + (1e-6*torch.eye(k)).cuda()\n",
    "    t1 = -0.5*(x - mean).view(1, k)@torch.inverse(cov)@(x - mean).view(k, 1)\n",
    "    #t21 = 0.5*k*torch.log(2*torch.tensor([math.pi]).cuda())\n",
    "    #t22 = 0.5*torch.log(torch.det(cov))\n",
    "    #t2 = t21 + t22\n",
    "    t2 = 0.5*k*torch.log(2*torch.tensor([math.pi]).cuda()) + 0.5*torch.log(torch.det(cov))\n",
    "    return t1 - t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3379]], device='cuda:0')\n",
      "--- 0.009974956512451172 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "mean = torch.tensor([0., 0.]).cuda()\n",
    "cov = torch.eye(2).cuda()\n",
    "x = torch.tensor([0., 1.]).cuda()\n",
    "print(multivariate_normal_logpdf(mean, cov, x))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.3379, device='cuda:0')\n",
      "--- 0.013963699340820312 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "mean = torch.tensor([0., 0.]).cuda()\n",
    "cov = torch.eye(2).cuda()\n",
    "x = torch.tensor([0., 1.]).cuda()\n",
    "m = MultivariateNormal(mean, cov)\n",
    "print(m.log_prob(x))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.3379, device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = MultivariateNormal(mean, cov)\n",
    "m.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor([[1.,1.],[2.,2.],[3.,3.]])\n",
    "print(z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, 784)\n",
    "    # single hidden layer\n",
    "    def forward(self, x):\n",
    "        #x = x.view(-1, 784)\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return F.sigmoid(self.fc2(h1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_z0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(q_z0, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 300)\n",
    "        self.fc2 = nn.Linear(300, 300)\n",
    "        self.fc31 = nn.Linear(300, z_dim)\n",
    "        self.fc32 = nn.Linear(300, z_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        h1 = F.softplus(self.fc1(x))\n",
    "        h2 = F.softplus(self.fc2(h1))\n",
    "        logvar = self.fc31(h2)\n",
    "        mu = self.fc32(h2)\n",
    "        return mu, logvar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class r_v(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(r_v, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim + 784, 300)\n",
    "        self.fc21 = nn.Linear(300, z_dim)\n",
    "        self.fc22 = nn.Linear(300, z_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784 + z_dim)\n",
    "        h1 = F.softplus(self.fc1(x))\n",
    "        logvar = self.fc21(h1)\n",
    "        mu = self.fc22(h1)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_v(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(q_v, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 300)\n",
    "        # no need to output mu because the mean of momentum is default 0\n",
    "        self.fc21 = nn.Linear(300, z_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        h1 = F.softplus(self.fc1(x))\n",
    "        logvar = self.fc21(h1)\n",
    "        return logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9189, -0.9189])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.zeros(2,1)\n",
    "cov = torch.eye(2)\n",
    "m = MultivariateNormal(mean, cov)\n",
    "x = torch.tensor([0.,0.]) \n",
    "m.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoder().to(device)\n",
    "q_z0 = q_z0().to(device)\n",
    "r_v = r_v().to(device)\n",
    "q_v = q_v().to(device)\n",
    "mass_diag = torch.randn(z_dim, requires_grad=True)\n",
    "#mass = torch.randn(z_dim, requires_grad=True)\n",
    "#mass = torch.eye(z_dim, requires_grad=True)\n",
    "#mass_cuda = mass.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mass = Variable(torch.randn(batch_size, 100).cuda(), requires_grad=True)\n",
    "#mass_diag = torch.randn(z_dim, requires_grad=True)\n",
    "#mass_matrix = torch.diag(mass_diag).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_bound(decoder, q_z0, r_v, q_v, data, mass_diag, T):\n",
    "    data = data.to(device)\n",
    "    mu_z0, logvar_z0 = q_z0(data)\n",
    "    var_z0 = torch.exp(logvar_z0)\n",
    "    #print(logvar_z0.shape)\n",
    "\n",
    "    # sample z0\n",
    "    z0 = reparameterize(mu_z0, logvar_z0)\n",
    "    #print(\"z0: \" + str(z0.shape))\n",
    "\n",
    "    # get joint probaility p(x, z0)\n",
    "    log_prior_z0 = log_prior(z0)\n",
    "    #print(\"log_prior_z0: \" + str(log_prior_z0.shape))\n",
    "    decoder_output = decoder(z0)\n",
    "    #print(\"decoder_output: \" + str(decoder_output.shape))\n",
    "    log_likelihood = 0. - F.binary_cross_entropy(decoder_output, data.view(-1, 784), size_average=False, reduce=False)\n",
    "    #print(\"log_likelihood: \" + str(log_likelihood.shape))\n",
    "    log_likelihood = torch.sum(log_likelihood, dim = 1)\n",
    "    #print(\"log_likelihood: \" + str(log_likelihood.shape))\n",
    "    log_joint = log_prior_z0 + log_likelihood\n",
    "    #print(\"log_joint: \" + str(log_joint.shape))\n",
    "\n",
    "    # get log q_z0\n",
    "    log_q_z0 = torch.zeros(0).cuda()\n",
    "    for i in range(batch_size):\n",
    "        one_cov = torch.diag(var_z0[i])\n",
    "        #m = MultivariateNormal(mu_z0[i], one_cov)\n",
    "        #one_q_z0 = m.log_prob(z0[i]).view(1)\n",
    "        one_q_z0 = multivariate_normal_logpdf(mu_z0[i], one_cov, z0[i])\n",
    "        #print(\"one q z0: \" + str(one_q_z0))\n",
    "        log_q_z0 = torch.cat((log_q_z0,one_q_z0),0)\n",
    "    #print(\"log_q_z0: \" + str(log_q_z0.shape))\n",
    "\n",
    "\n",
    "    # initial L for 128 samples\n",
    "    L = log_joint - log_q_z0\n",
    "    L = torch.sum(L)\n",
    "    #print(\"L \"+str(L))\n",
    "    #print(L.shape)\n",
    "\n",
    "    #print(\"====================================\")\n",
    "    for i in range(T):\n",
    "         # sample v_t'(v1)\n",
    "        logvar_v1 = q_v(data)\n",
    "        var_v1 = torch.exp(logvar_v1)\n",
    "        #print(\"logvar_v1: \" + str(logvar_v1.shape))\n",
    "        mu_v1 = torch.zeros(logvar_v1.shape[0], logvar_v1.shape[1]).cuda()\n",
    "        v1 = reparameterize(mu_v1, logvar_v1)\n",
    "        #print(\"v1: \"+str(v1.shape))\n",
    "\n",
    "        # get q_v1\n",
    "        log_q_v1 = torch.zeros(0).cuda()\n",
    "        for i in range(batch_size):\n",
    "            one_cov = torch.diag(var_v1[i])\n",
    "            #m = MultivariateNormal(mu_v1[i], one_cov)\n",
    "            #one_q_v1 = m.log_prob(v1[i]).view(1)\n",
    "            one_q_v1 = multivariate_normal_logpdf(mu_v1[i], one_cov, v1[i])\n",
    "            log_q_v1 = torch.cat((log_q_v1,one_q_v1),0)\n",
    "        #print(\"log_q_v1: \"+str(log_q_v1.shape))\n",
    "\n",
    "        log_joint_t = torch.zeros(0).cuda() # list of all the joint\n",
    "        log_r_vt = torch.zeros(0).cuda()\n",
    "        alpha = torch.tensor([0.]).cuda() # lower bound for each batch (128 samples)\n",
    "        for j in range(batch_size):\n",
    "            def energy_function(z, cache):\n",
    "                z.retain_grad()\n",
    "                z = z.view(1, z.shape[0])\n",
    "                z = z.cuda()\n",
    "                one_log_prior = log_prior(z)\n",
    "                decoder_output = decoder(z)\n",
    "                one_log_likelihood = 0. - F.binary_cross_entropy(decoder_output, data.view(-1, 784)[j], size_average=False, reduce=False)\n",
    "                #print(one_log_likelihood.shape)\n",
    "                one_log_likelihood = torch.sum(one_log_likelihood, dim = 1)\n",
    "                one_log_joint = one_log_prior + one_log_likelihood\n",
    "                return 0 - one_log_joint\n",
    "            sampler = IsotropicHmcSampler(energy_function, energy_grad=None, prng=None,\n",
    "                                          mom_resample_coeff=1., dtype=np.float64)\n",
    "            init = torch.zeros(z_dim).cuda()\n",
    "            mass_matrix = torch.diag(mass_diag)\n",
    "            mass_matrix.cuda()\n",
    "            pos_samples, mom_samples, ratio = sampler.get_samples(init, 0.1, 3, 2, mass_matrix, mom = v1[j].view(z_dim))\n",
    "            #print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`\")\n",
    "            #print(pos_samples[1].shape)\n",
    "\n",
    "            # get joint probaility p(x, zt)\n",
    "            zt = pos_samples[1].cuda()\n",
    "            vt = mom_samples[1].cuda()\n",
    "            zt = zt.view(1, zt.shape[0])\n",
    "            vt = vt.view(vt.shape[0])\n",
    "\n",
    "            # get joint probaility p(x, zt)\n",
    "            one_log_prior_zt = log_prior(zt)\n",
    "            #print(\"one_log_prior_zt: \" + str(one_log_prior_zt.shape))\n",
    "            one_decoder_output_t = decoder(zt)\n",
    "            #print(\"one_decoder_output_t: \" + str(one_decoder_output_t.shape))\n",
    "            one_log_likelihood_t = 0. - F.binary_cross_entropy(one_decoder_output_t, data.view(-1, 784)[j], size_average=False, reduce=False)\n",
    "            one_log_likelihood_t = torch.sum(one_log_likelihood_t, dim = 1)\n",
    "            #print(\"one_log_likelihood_t: \" + str(one_log_likelihood_t.shape))\n",
    "            one_log_joint_t = one_log_prior_zt + one_log_likelihood_t\n",
    "            #print(\"one_log_joint_t: \" + str(one_log_joint_t.shape))\n",
    "            log_joint_t = torch.cat((log_joint_t, one_log_joint_t), 0)\n",
    "\n",
    "            # get r_vt\n",
    "            d = data.view(-1, 784)[j].view(1, 784)\n",
    "            one_new_data = torch.cat((d, zt), 1) # append data with zt\n",
    "            one_mu_vt, one_logvar_vt = r_v(one_new_data)\n",
    "            one_var_vt = torch.exp(one_logvar_vt)\n",
    "            one_mu_vt = one_mu_vt.view(one_mu_vt.shape[1])\n",
    "            one_cov = torch.diag(one_var_vt.view(one_var_vt.shape[1]))\n",
    "            #m = MultivariateNormal(one_mu_vt, one_cov)\n",
    "            #one_log_r_vt = m.log_prob(vt).view(1)\n",
    "            one_log_r_vt = multivariate_normal_logpdf(one_mu_vt, one_cov, vt)\n",
    "            log_r_vt = torch.cat((log_r_vt, one_log_r_vt), 0)\n",
    "            \n",
    "\n",
    "            # get L for each sample\n",
    "            one_log_alpha = log_joint_t[j] + log_r_vt[j] - log_joint[j] - log_q_v1[j]\n",
    "            #print(\"one log alpha: \"+str(one_log_alpha))\n",
    "            #one_log_alpha = torch.log(one_alpha)\n",
    "            L = L + one_log_alpha\n",
    "            #alpha = alpha + one_alpha\n",
    "        #L = L + torch.log(alpha)\n",
    "\n",
    "    #print(\"~~~~~~~~~~~~~~~~~~~ new L \" + str(L) + \" ~~~~~~~~~~~~~~~~~~~\")\n",
    "    return L    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can't optimize a non-leaf Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-eb57e455ec9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mparams1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_z0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmass_diag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moptimizer1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#optimizer2 = optim.Adam([mass_diag], lr=1e-3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[0;32m     39\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[0;32m     40\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36madd_param_group\u001b[1;34m(self, param_group)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"optimizing a parameter that doesn't require gradients\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"can't optimize a non-leaf Tensor\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: can't optimize a non-leaf Tensor"
     ]
    }
   ],
   "source": [
    "params1 = list(decoder.parameters())+list(q_z0.parameters())+list(r_v.parameters())+list(q_v.parameters())+list(mass_diag)\n",
    "optimizer1 = optim.Adam(params1, lr=1e-3)\n",
    "#optimizer2 = optim.Adam([mass_diag], lr=1e-3)\n",
    "\n",
    "for batch_idx, (data, _) in enumerate(train_loader):\n",
    "    print(\"++++++++++ \" + str(batch_idx) + \" ++++++++++\")\n",
    "    optimizer1.zero_grad()\n",
    "    #optimizer2.zero_grad()\n",
    "    L = lower_bound(decoder, q_z0, r_v, q_v, data, mass_diag, 1)\n",
    "    loss = 0. - L\n",
    "    loss.backward()\n",
    "    optimizer1.step()\n",
    "    #optimizer2.step()\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++ 0 ++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1474: UserWarning: Using a target size (torch.Size([784])) that is different to the input size (torch.Size([1, 784])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++ 1 ++++++++++\n",
      "++++++++++ 2 ++++++++++\n",
      "++++++++++ 3 ++++++++++\n",
      "++++++++++ 4 ++++++++++\n",
      "++++++++++ 5 ++++++++++\n",
      "++++++++++ 6 ++++++++++\n",
      "++++++++++ 7 ++++++++++\n",
      "++++++++++ 8 ++++++++++\n",
      "++++++++++ 9 ++++++++++\n",
      "++++++++++ 10 ++++++++++\n",
      "++++++++++ 11 ++++++++++\n",
      "++++++++++ 12 ++++++++++\n",
      "++++++++++ 13 ++++++++++\n",
      "++++++++++ 14 ++++++++++\n",
      "++++++++++ 15 ++++++++++\n",
      "++++++++++ 16 ++++++++++\n",
      "++++++++++ 17 ++++++++++\n",
      "++++++++++ 18 ++++++++++\n",
      "++++++++++ 19 ++++++++++\n",
      "++++++++++ 20 ++++++++++\n",
      "++++++++++ 21 ++++++++++\n",
      "++++++++++ 22 ++++++++++\n",
      "++++++++++ 23 ++++++++++\n",
      "++++++++++ 24 ++++++++++\n",
      "++++++++++ 25 ++++++++++\n",
      "++++++++++ 26 ++++++++++\n",
      "++++++++++ 27 ++++++++++\n",
      "++++++++++ 28 ++++++++++\n",
      "++++++++++ 29 ++++++++++\n",
      "++++++++++ 30 ++++++++++\n",
      "++++++++++ 31 ++++++++++\n",
      "++++++++++ 32 ++++++++++\n",
      "++++++++++ 33 ++++++++++\n",
      "++++++++++ 34 ++++++++++\n",
      "++++++++++ 35 ++++++++++\n",
      "++++++++++ 36 ++++++++++\n",
      "++++++++++ 37 ++++++++++\n",
      "++++++++++ 38 ++++++++++\n",
      "++++++++++ 39 ++++++++++\n",
      "++++++++++ 40 ++++++++++\n",
      "++++++++++ 41 ++++++++++\n",
      "++++++++++ 42 ++++++++++\n",
      "++++++++++ 43 ++++++++++\n",
      "++++++++++ 44 ++++++++++\n",
      "++++++++++ 45 ++++++++++\n",
      "++++++++++ 46 ++++++++++\n",
      "++++++++++ 47 ++++++++++\n",
      "++++++++++ 48 ++++++++++\n",
      "++++++++++ 49 ++++++++++\n",
      "++++++++++ 50 ++++++++++\n",
      "++++++++++ 51 ++++++++++\n",
      "++++++++++ 52 ++++++++++\n",
      "++++++++++ 53 ++++++++++\n",
      "++++++++++ 54 ++++++++++\n",
      "++++++++++ 55 ++++++++++\n",
      "++++++++++ 56 ++++++++++\n",
      "++++++++++ 57 ++++++++++\n",
      "++++++++++ 58 ++++++++++\n",
      "++++++++++ 59 ++++++++++\n",
      "++++++++++ 60 ++++++++++\n",
      "++++++++++ 61 ++++++++++\n",
      "++++++++++ 62 ++++++++++\n",
      "++++++++++ 63 ++++++++++\n",
      "++++++++++ 64 ++++++++++\n",
      "++++++++++ 65 ++++++++++\n",
      "++++++++++ 66 ++++++++++\n",
      "++++++++++ 67 ++++++++++\n",
      "++++++++++ 68 ++++++++++\n",
      "++++++++++ 69 ++++++++++\n",
      "++++++++++ 70 ++++++++++\n",
      "++++++++++ 71 ++++++++++\n",
      "++++++++++ 72 ++++++++++\n",
      "++++++++++ 73 ++++++++++\n",
      "++++++++++ 74 ++++++++++\n",
      "++++++++++ 75 ++++++++++\n",
      "++++++++++ 76 ++++++++++\n",
      "++++++++++ 77 ++++++++++\n",
      "++++++++++ 78 ++++++++++\n",
      "++++++++++ 79 ++++++++++\n",
      "++++++++++ 80 ++++++++++\n",
      "++++++++++ 81 ++++++++++\n",
      "++++++++++ 82 ++++++++++\n",
      "++++++++++ 83 ++++++++++\n",
      "++++++++++ 84 ++++++++++\n",
      "++++++++++ 85 ++++++++++\n",
      "++++++++++ 86 ++++++++++\n",
      "++++++++++ 87 ++++++++++\n",
      "++++++++++ 88 ++++++++++\n",
      "++++++++++ 89 ++++++++++\n",
      "++++++++++ 90 ++++++++++\n",
      "++++++++++ 91 ++++++++++\n",
      "++++++++++ 92 ++++++++++\n",
      "++++++++++ 93 ++++++++++\n",
      "++++++++++ 94 ++++++++++\n",
      "++++++++++ 95 ++++++++++\n",
      "++++++++++ 96 ++++++++++\n",
      "++++++++++ 97 ++++++++++\n",
      "++++++++++ 98 ++++++++++\n",
      "++++++++++ 99 ++++++++++\n",
      "++++++++++ 100 ++++++++++\n",
      "++++++++++ 101 ++++++++++\n",
      "++++++++++ 102 ++++++++++\n",
      "++++++++++ 103 ++++++++++\n",
      "++++++++++ 104 ++++++++++\n",
      "++++++++++ 105 ++++++++++\n",
      "++++++++++ 106 ++++++++++\n",
      "++++++++++ 107 ++++++++++\n",
      "++++++++++ 108 ++++++++++\n",
      "++++++++++ 109 ++++++++++\n",
      "++++++++++ 110 ++++++++++\n",
      "++++++++++ 111 ++++++++++\n",
      "++++++++++ 112 ++++++++++\n",
      "++++++++++ 113 ++++++++++\n",
      "++++++++++ 114 ++++++++++\n",
      "++++++++++ 115 ++++++++++\n",
      "++++++++++ 116 ++++++++++\n",
      "++++++++++ 117 ++++++++++\n",
      "++++++++++ 118 ++++++++++\n",
      "++++++++++ 119 ++++++++++\n",
      "++++++++++ 120 ++++++++++\n",
      "++++++++++ 121 ++++++++++\n",
      "++++++++++ 122 ++++++++++\n",
      "++++++++++ 123 ++++++++++\n",
      "++++++++++ 124 ++++++++++\n",
      "++++++++++ 125 ++++++++++\n",
      "++++++++++ 126 ++++++++++\n",
      "++++++++++ 127 ++++++++++\n",
      "++++++++++ 128 ++++++++++\n",
      "++++++++++ 129 ++++++++++\n",
      "++++++++++ 130 ++++++++++\n",
      "++++++++++ 131 ++++++++++\n",
      "++++++++++ 132 ++++++++++\n",
      "++++++++++ 133 ++++++++++\n",
      "++++++++++ 134 ++++++++++\n",
      "++++++++++ 135 ++++++++++\n",
      "++++++++++ 136 ++++++++++\n",
      "++++++++++ 137 ++++++++++\n",
      "++++++++++ 138 ++++++++++\n",
      "++++++++++ 139 ++++++++++\n",
      "++++++++++ 140 ++++++++++\n",
      "++++++++++ 141 ++++++++++\n",
      "++++++++++ 142 ++++++++++\n",
      "++++++++++ 143 ++++++++++\n",
      "++++++++++ 144 ++++++++++\n",
      "++++++++++ 145 ++++++++++\n",
      "++++++++++ 146 ++++++++++\n",
      "++++++++++ 147 ++++++++++\n",
      "++++++++++ 148 ++++++++++\n",
      "++++++++++ 149 ++++++++++\n",
      "++++++++++ 150 ++++++++++\n",
      "++++++++++ 151 ++++++++++\n",
      "++++++++++ 152 ++++++++++\n",
      "++++++++++ 153 ++++++++++\n",
      "++++++++++ 154 ++++++++++\n",
      "++++++++++ 155 ++++++++++\n",
      "++++++++++ 156 ++++++++++\n",
      "++++++++++ 157 ++++++++++\n",
      "++++++++++ 158 ++++++++++\n",
      "++++++++++ 159 ++++++++++\n",
      "++++++++++ 160 ++++++++++\n",
      "++++++++++ 161 ++++++++++\n",
      "++++++++++ 162 ++++++++++\n",
      "++++++++++ 163 ++++++++++\n",
      "++++++++++ 164 ++++++++++\n",
      "++++++++++ 165 ++++++++++\n",
      "++++++++++ 166 ++++++++++\n",
      "++++++++++ 167 ++++++++++\n",
      "++++++++++ 168 ++++++++++\n",
      "++++++++++ 169 ++++++++++\n",
      "++++++++++ 170 ++++++++++\n",
      "++++++++++ 171 ++++++++++\n",
      "++++++++++ 172 ++++++++++\n",
      "++++++++++ 173 ++++++++++\n",
      "++++++++++ 174 ++++++++++\n",
      "++++++++++ 175 ++++++++++\n",
      "++++++++++ 176 ++++++++++\n",
      "++++++++++ 177 ++++++++++\n",
      "++++++++++ 178 ++++++++++\n",
      "++++++++++ 179 ++++++++++\n",
      "++++++++++ 180 ++++++++++\n",
      "++++++++++ 181 ++++++++++\n",
      "++++++++++ 182 ++++++++++\n",
      "++++++++++ 183 ++++++++++\n",
      "++++++++++ 184 ++++++++++\n",
      "++++++++++ 185 ++++++++++\n",
      "++++++++++ 186 ++++++++++\n",
      "++++++++++ 187 ++++++++++\n",
      "++++++++++ 188 ++++++++++\n",
      "++++++++++ 189 ++++++++++\n",
      "++++++++++ 190 ++++++++++\n",
      "++++++++++ 191 ++++++++++\n",
      "++++++++++ 192 ++++++++++\n",
      "++++++++++ 193 ++++++++++\n",
      "++++++++++ 194 ++++++++++\n",
      "++++++++++ 195 ++++++++++\n",
      "++++++++++ 196 ++++++++++\n",
      "++++++++++ 197 ++++++++++\n",
      "++++++++++ 198 ++++++++++\n",
      "++++++++++ 199 ++++++++++\n",
      "++++++++++ 200 ++++++++++\n",
      "++++++++++ 201 ++++++++++\n",
      "++++++++++ 202 ++++++++++\n",
      "++++++++++ 203 ++++++++++\n",
      "++++++++++ 204 ++++++++++\n",
      "++++++++++ 205 ++++++++++\n",
      "++++++++++ 206 ++++++++++\n",
      "++++++++++ 207 ++++++++++\n",
      "++++++++++ 208 ++++++++++\n",
      "++++++++++ 209 ++++++++++\n",
      "++++++++++ 210 ++++++++++\n",
      "++++++++++ 211 ++++++++++\n",
      "++++++++++ 212 ++++++++++\n",
      "++++++++++ 213 ++++++++++\n",
      "++++++++++ 214 ++++++++++\n",
      "++++++++++ 215 ++++++++++\n",
      "++++++++++ 216 ++++++++++\n",
      "++++++++++ 217 ++++++++++\n",
      "++++++++++ 218 ++++++++++\n",
      "++++++++++ 219 ++++++++++\n",
      "++++++++++ 220 ++++++++++\n",
      "++++++++++ 221 ++++++++++\n",
      "++++++++++ 222 ++++++++++\n",
      "++++++++++ 223 ++++++++++\n",
      "++++++++++ 224 ++++++++++\n",
      "++++++++++ 225 ++++++++++\n",
      "++++++++++ 226 ++++++++++\n",
      "++++++++++ 227 ++++++++++\n",
      "++++++++++ 228 ++++++++++\n",
      "++++++++++ 229 ++++++++++\n",
      "++++++++++ 230 ++++++++++\n",
      "++++++++++ 231 ++++++++++\n",
      "++++++++++ 232 ++++++++++\n",
      "++++++++++ 233 ++++++++++\n",
      "++++++++++ 234 ++++++++++\n",
      "++++++++++ 235 ++++++++++\n",
      "++++++++++ 236 ++++++++++\n",
      "++++++++++ 237 ++++++++++\n",
      "++++++++++ 238 ++++++++++\n",
      "++++++++++ 239 ++++++++++\n",
      "++++++++++ 240 ++++++++++\n",
      "++++++++++ 241 ++++++++++\n",
      "++++++++++ 242 ++++++++++\n",
      "++++++++++ 243 ++++++++++\n",
      "++++++++++ 244 ++++++++++\n",
      "++++++++++ 245 ++++++++++\n",
      "++++++++++ 246 ++++++++++\n",
      "++++++++++ 247 ++++++++++\n",
      "++++++++++ 248 ++++++++++\n",
      "++++++++++ 249 ++++++++++\n",
      "++++++++++ 250 ++++++++++\n",
      "++++++++++ 251 ++++++++++\n",
      "++++++++++ 252 ++++++++++\n",
      "++++++++++ 253 ++++++++++\n",
      "++++++++++ 254 ++++++++++\n",
      "++++++++++ 255 ++++++++++\n",
      "++++++++++ 256 ++++++++++\n",
      "++++++++++ 257 ++++++++++\n",
      "++++++++++ 258 ++++++++++\n",
      "++++++++++ 259 ++++++++++\n",
      "++++++++++ 260 ++++++++++\n",
      "++++++++++ 261 ++++++++++\n",
      "++++++++++ 262 ++++++++++\n",
      "++++++++++ 263 ++++++++++\n",
      "++++++++++ 264 ++++++++++\n",
      "++++++++++ 265 ++++++++++\n",
      "++++++++++ 266 ++++++++++\n",
      "++++++++++ 267 ++++++++++\n",
      "++++++++++ 268 ++++++++++\n",
      "++++++++++ 269 ++++++++++\n",
      "++++++++++ 270 ++++++++++\n",
      "++++++++++ 271 ++++++++++\n",
      "++++++++++ 272 ++++++++++\n",
      "++++++++++ 273 ++++++++++\n",
      "++++++++++ 274 ++++++++++\n",
      "++++++++++ 275 ++++++++++\n",
      "++++++++++ 276 ++++++++++\n",
      "++++++++++ 277 ++++++++++\n",
      "++++++++++ 278 ++++++++++\n",
      "++++++++++ 279 ++++++++++\n",
      "++++++++++ 280 ++++++++++\n",
      "++++++++++ 281 ++++++++++\n",
      "++++++++++ 282 ++++++++++\n",
      "++++++++++ 283 ++++++++++\n",
      "++++++++++ 284 ++++++++++\n",
      "++++++++++ 285 ++++++++++\n",
      "++++++++++ 286 ++++++++++\n",
      "++++++++++ 287 ++++++++++\n",
      "++++++++++ 288 ++++++++++\n",
      "++++++++++ 289 ++++++++++\n",
      "++++++++++ 290 ++++++++++\n",
      "++++++++++ 291 ++++++++++\n",
      "++++++++++ 292 ++++++++++\n",
      "++++++++++ 293 ++++++++++\n",
      "++++++++++ 294 ++++++++++\n",
      "++++++++++ 295 ++++++++++\n",
      "++++++++++ 296 ++++++++++\n",
      "++++++++++ 297 ++++++++++\n",
      "++++++++++ 298 ++++++++++\n",
      "++++++++++ 299 ++++++++++\n",
      "++++++++++ 300 ++++++++++\n",
      "++++++++++ 301 ++++++++++\n",
      "++++++++++ 302 ++++++++++\n",
      "++++++++++ 303 ++++++++++\n",
      "++++++++++ 304 ++++++++++\n",
      "++++++++++ 305 ++++++++++\n",
      "++++++++++ 306 ++++++++++\n",
      "++++++++++ 307 ++++++++++\n",
      "++++++++++ 308 ++++++++++\n",
      "++++++++++ 309 ++++++++++\n",
      "++++++++++ 310 ++++++++++\n",
      "++++++++++ 311 ++++++++++\n",
      "++++++++++ 312 ++++++++++\n",
      "++++++++++ 313 ++++++++++\n",
      "++++++++++ 314 ++++++++++\n",
      "++++++++++ 315 ++++++++++\n",
      "++++++++++ 316 ++++++++++\n",
      "++++++++++ 317 ++++++++++\n",
      "++++++++++ 318 ++++++++++\n",
      "++++++++++ 319 ++++++++++\n",
      "++++++++++ 320 ++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++ 321 ++++++++++\n",
      "++++++++++ 322 ++++++++++\n",
      "++++++++++ 323 ++++++++++\n",
      "++++++++++ 324 ++++++++++\n",
      "++++++++++ 325 ++++++++++\n",
      "++++++++++ 326 ++++++++++\n",
      "++++++++++ 327 ++++++++++\n",
      "++++++++++ 328 ++++++++++\n",
      "++++++++++ 329 ++++++++++\n",
      "++++++++++ 330 ++++++++++\n",
      "++++++++++ 331 ++++++++++\n",
      "++++++++++ 332 ++++++++++\n",
      "++++++++++ 333 ++++++++++\n",
      "++++++++++ 334 ++++++++++\n",
      "++++++++++ 335 ++++++++++\n",
      "++++++++++ 336 ++++++++++\n",
      "++++++++++ 337 ++++++++++\n",
      "++++++++++ 338 ++++++++++\n",
      "++++++++++ 339 ++++++++++\n",
      "++++++++++ 340 ++++++++++\n",
      "++++++++++ 341 ++++++++++\n",
      "++++++++++ 342 ++++++++++\n",
      "++++++++++ 343 ++++++++++\n",
      "++++++++++ 344 ++++++++++\n",
      "++++++++++ 345 ++++++++++\n",
      "++++++++++ 346 ++++++++++\n",
      "++++++++++ 347 ++++++++++\n",
      "++++++++++ 348 ++++++++++\n",
      "++++++++++ 349 ++++++++++\n",
      "++++++++++ 350 ++++++++++\n",
      "++++++++++ 351 ++++++++++\n",
      "++++++++++ 352 ++++++++++\n",
      "++++++++++ 353 ++++++++++\n",
      "++++++++++ 354 ++++++++++\n",
      "++++++++++ 355 ++++++++++\n",
      "++++++++++ 356 ++++++++++\n",
      "++++++++++ 357 ++++++++++\n",
      "++++++++++ 358 ++++++++++\n",
      "++++++++++ 359 ++++++++++\n",
      "++++++++++ 360 ++++++++++\n",
      "++++++++++ 361 ++++++++++\n",
      "++++++++++ 362 ++++++++++\n",
      "++++++++++ 363 ++++++++++\n",
      "++++++++++ 364 ++++++++++\n",
      "++++++++++ 365 ++++++++++\n",
      "++++++++++ 366 ++++++++++\n",
      "++++++++++ 367 ++++++++++\n",
      "++++++++++ 368 ++++++++++\n",
      "++++++++++ 369 ++++++++++\n",
      "++++++++++ 370 ++++++++++\n",
      "++++++++++ 371 ++++++++++\n",
      "++++++++++ 372 ++++++++++\n",
      "++++++++++ 373 ++++++++++\n",
      "++++++++++ 374 ++++++++++\n",
      "++++++++++ 375 ++++++++++\n",
      "++++++++++ 376 ++++++++++\n",
      "++++++++++ 377 ++++++++++\n",
      "++++++++++ 378 ++++++++++\n",
      "++++++++++ 379 ++++++++++\n",
      "++++++++++ 380 ++++++++++\n",
      "++++++++++ 381 ++++++++++\n",
      "++++++++++ 382 ++++++++++\n",
      "++++++++++ 383 ++++++++++\n",
      "++++++++++ 384 ++++++++++\n",
      "++++++++++ 385 ++++++++++\n",
      "++++++++++ 386 ++++++++++\n",
      "++++++++++ 387 ++++++++++\n",
      "++++++++++ 388 ++++++++++\n",
      "++++++++++ 389 ++++++++++\n",
      "++++++++++ 390 ++++++++++\n",
      "++++++++++ 391 ++++++++++\n",
      "++++++++++ 392 ++++++++++\n",
      "++++++++++ 393 ++++++++++\n",
      "++++++++++ 394 ++++++++++\n",
      "++++++++++ 395 ++++++++++\n",
      "++++++++++ 396 ++++++++++\n",
      "++++++++++ 397 ++++++++++\n",
      "++++++++++ 398 ++++++++++\n",
      "++++++++++ 399 ++++++++++\n",
      "++++++++++ 400 ++++++++++\n",
      "++++++++++ 401 ++++++++++\n",
      "++++++++++ 402 ++++++++++\n",
      "++++++++++ 403 ++++++++++\n",
      "++++++++++ 404 ++++++++++\n",
      "++++++++++ 405 ++++++++++\n",
      "++++++++++ 406 ++++++++++\n",
      "++++++++++ 407 ++++++++++\n",
      "++++++++++ 408 ++++++++++\n",
      "++++++++++ 409 ++++++++++\n",
      "++++++++++ 410 ++++++++++\n",
      "++++++++++ 411 ++++++++++\n",
      "++++++++++ 412 ++++++++++\n",
      "++++++++++ 413 ++++++++++\n",
      "++++++++++ 414 ++++++++++\n",
      "++++++++++ 415 ++++++++++\n",
      "++++++++++ 416 ++++++++++\n",
      "++++++++++ 417 ++++++++++\n",
      "++++++++++ 418 ++++++++++\n",
      "++++++++++ 419 ++++++++++\n",
      "++++++++++ 420 ++++++++++\n",
      "++++++++++ 421 ++++++++++\n",
      "++++++++++ 422 ++++++++++\n",
      "++++++++++ 423 ++++++++++\n",
      "++++++++++ 424 ++++++++++\n",
      "++++++++++ 425 ++++++++++\n",
      "++++++++++ 426 ++++++++++\n",
      "++++++++++ 427 ++++++++++\n",
      "++++++++++ 428 ++++++++++\n",
      "++++++++++ 429 ++++++++++\n",
      "++++++++++ 430 ++++++++++\n",
      "++++++++++ 431 ++++++++++\n",
      "++++++++++ 432 ++++++++++\n",
      "++++++++++ 433 ++++++++++\n",
      "++++++++++ 434 ++++++++++\n",
      "++++++++++ 435 ++++++++++\n",
      "++++++++++ 436 ++++++++++\n",
      "++++++++++ 437 ++++++++++\n",
      "++++++++++ 438 ++++++++++\n",
      "++++++++++ 439 ++++++++++\n",
      "++++++++++ 440 ++++++++++\n",
      "++++++++++ 441 ++++++++++\n",
      "++++++++++ 442 ++++++++++\n",
      "++++++++++ 443 ++++++++++\n",
      "++++++++++ 444 ++++++++++\n",
      "++++++++++ 445 ++++++++++\n",
      "++++++++++ 446 ++++++++++\n",
      "++++++++++ 447 ++++++++++\n",
      "++++++++++ 448 ++++++++++\n",
      "++++++++++ 449 ++++++++++\n",
      "++++++++++ 450 ++++++++++\n",
      "++++++++++ 451 ++++++++++\n",
      "++++++++++ 452 ++++++++++\n",
      "++++++++++ 453 ++++++++++\n",
      "++++++++++ 454 ++++++++++\n",
      "++++++++++ 455 ++++++++++\n",
      "++++++++++ 456 ++++++++++\n",
      "++++++++++ 457 ++++++++++\n",
      "++++++++++ 458 ++++++++++\n",
      "++++++++++ 459 ++++++++++\n",
      "++++++++++ 460 ++++++++++\n",
      "++++++++++ 461 ++++++++++\n",
      "++++++++++ 462 ++++++++++\n",
      "++++++++++ 463 ++++++++++\n",
      "++++++++++ 464 ++++++++++\n",
      "++++++++++ 465 ++++++++++\n",
      "++++++++++ 466 ++++++++++\n",
      "++++++++++ 467 ++++++++++\n",
      "++++++++++ 468 ++++++++++\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 96 is out of bounds for dimension 0 with size 96",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8c87df58302b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0moptimizer1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0moptimizer2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_z0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmass_diag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-53e7c0a687cd>\u001b[0m in \u001b[0;36mlower_bound\u001b[1;34m(decoder, q_z0, r_v, q_v, data, mass_diag, T)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mlog_q_z0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mone_cov\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_z0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;31m#m = MultivariateNormal(mu_z0[i], one_cov)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m#one_q_z0 = m.log_prob(z0[i]).view(1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 96 is out of bounds for dimension 0 with size 96"
     ]
    }
   ],
   "source": [
    "params1 = list(decoder.parameters())+list(q_z0.parameters())+list(r_v.parameters())+list(q_v.parameters())\n",
    "optimizer1 = optim.Adam(params1, lr=1e-3)\n",
    "optimizer2 = optim.Adam([mass_diag], lr=1e-3)\n",
    "\n",
    "for batch_idx, (data, _) in enumerate(train_loader):\n",
    "    print(\"++++++++++ \" + str(batch_idx) + \" ++++++++++\")\n",
    "    optimizer1.zero_grad()\n",
    "    optimizer2.zero_grad()\n",
    "    L = lower_bound(decoder, q_z0, r_v, q_v, data, mass_diag, 1)\n",
    "    loss = 0. - L\n",
    "    loss.backward()\n",
    "    optimizer1.step()\n",
    "    optimizer2.step()\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e+06 *\n",
       "       [-2.2254], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "params argument given to the optimizer should be an iterable of Tensors or dicts, but got torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-143b7071f5f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#x = Variable(torch.randn(5))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\optim\\sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, lr, momentum, dampening, weight_decay, nesterov)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdampening\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Nesterov momentum requires a momentum and zero dampening\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m     29\u001b[0m             raise TypeError(\"params argument given to the optimizer should be \"\n\u001b[0;32m     30\u001b[0m                             \u001b[1;34m\"an iterable of Tensors or dicts, but got \"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                             torch.typename(params))\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: params argument given to the optimizer should be an iterable of Tensors or dicts, but got torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "#params = mass_diag\n",
    "\n",
    "\n",
    "#optimizer = optim.Adam(params, lr=1e-3)\n",
    "\n",
    "w = torch.randn([3,5], requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "#x = Variable(torch.randn(5))\n",
    "\n",
    "optimizer = optim.SGD([w], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 20])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(decoder.parameters())\n",
    "params[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(decoder.parameters())[0]\n",
    "#list(mass_diag)[0]\n",
    "mass_diag.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = torch.ones(1, 784).cuda()\n",
    "print(tt.shape)\n",
    "torch.sum(tt, dim=1)\n",
    "tt.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0.1, 0.1],[0.1,0.1],[0.1,0.1]])\n",
    "y = torch.tensor([[0.5, 0.5],[0.5,0.5],[1.,1.]])\n",
    "L = F.binary_cross_entropy(y, x, size_average=False, reduce=False)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Lapack Error getrf : U(2,2) is 0, U is singular at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524546354046\\work\\aten\\src\\th\\generic/THTensorLapack.c:514",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-dee977365fee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Lapack Error getrf : U(2,2) is 0, U is singular at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524546354046\\work\\aten\\src\\th\\generic/THTensorLapack.c:514"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0.1, 0.1],[0.1,0.1]])\n",
    "torch.inverse(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.3863,   1.3863,  49.7358])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(L, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  0,  0],\n",
       "        [ 0,  2,  0],\n",
       "        [ 0,  0,  3]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(torch.tensor([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0050])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(torch.tensor([0.]),torch.tensor([1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.],\n",
       "        [ 0.,  1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3646,  0.7910,  0.5176]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.view(1, 3)\n",
    "y.view(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for i in range(500):\n",
    "    x.append(reparameterize(torch.tensor([0.]), torch.tensor([1.])).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a17b3eba90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
