{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('F:\\Cambridge\\Project\\MHMC-for-VAE\\change_of_variable')\n",
    "sys.path.append('F:\\Cambridge\\Project\\MHMC-for-VAE\\hmc_pytorch')\n",
    "from change_of_variable_pytorch import * \n",
    "from hmc_base_pytorch import *\n",
    "from hmc_unconstrained_pytorch import *\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(z):\n",
    "    dim = z.shape[0]\n",
    "    mean = torch.zeros(dim).cuda()\n",
    "    cov = 2*torch.eye(dim).cuda()\n",
    "    m = MultivariateNormal(mean, cov)\n",
    "    m.requires_grad=True\n",
    "    return m.log_prob(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_normal_diagonal_logpdf(mean, cov_diag, x):\n",
    "     # number of samples\n",
    "    k = x.shape[0] # dimension\n",
    "    #print(x.is_cuda)\n",
    "    #print(mean.is_cuda)\n",
    "    #print(cov_diag.is_cuda)\n",
    "    t1 = -0.5*(x - mean)*(1/cov_diag)*(x-mean)\n",
    "    t1 = torch.sum(t1, dim=1)\n",
    "    t2 = 0.5*k*torch.log(torch.tensor([2*math.pi]).cuda()) + 0.5*torch.sum(torch.log(cov_diag),dim=1)\n",
    "    return t1 - t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.5310], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_cuda = torch.zeros(2, requires_grad=True).cuda()\n",
    "mean = torch.zeros([1,2], requires_grad=True).cuda()\n",
    "cov_diag = 2*torch.ones([1,2], requires_grad=True).cuda()\n",
    "-multivariate_normal_diagonal_logpdf(mean, cov_diag, z_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.5310, device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prior(z_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(z, cache):\n",
    "    #z.retain_grad()\n",
    "    z_cuda = z.cuda()\n",
    "    mean = torch.zeros([1,2], requires_grad=True).cuda()\n",
    "    cov_diag = torch.ones([1,2], requires_grad=True).cuda()\n",
    "    res = -multivariate_normal_diagonal_logpdf(mean, cov_diag, z_cuda)\n",
    "    print(\"energy: \"+str(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = IsotropicHmcSampler(energy, energy_grad=None, prng=None,\n",
    "                                          mom_resample_coeff=1., dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy: tensor([ 1.8379], device='cuda:0')\n",
      "energy: tensor([ 1.8379], device='cuda:0')\n",
      "pos: tensor([ 0.,  0.])\n",
      "grad: tensor([ 0.,  0.])\n",
      "energy: tensor([ 1.8379], device='cuda:0')\n",
      "loop grad: tensor([ 0.,  0.], device='cuda:0')\n",
      "energy: tensor([ 1.8380], device='cuda:0')\n",
      "loop grad: tensor([ 0.,  0.], device='cuda:0')\n",
      "energy: tensor([ 1.8381], device='cuda:0')\n",
      "loop grad: tensor([ 0.,  0.], device='cuda:0')\n",
      "energy: tensor([ 1.8383], device='cuda:0')\n",
      "loop grad: tensor([ 0.,  0.], device='cuda:0')\n",
      "energy: tensor([ 1.8385], device='cuda:0')\n",
      "energy: tensor([ 1.8385], device='cuda:0')\n",
      "mom: tensor([-1.6793, -0.5392])\n",
      "energy: tensor([ 1.8385], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-02 *\n",
       "       [[ 0.0000,  0.0000],\n",
       "        [ 2.5000,  2.5000]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = torch.zeros(2, requires_grad=True).cuda()\n",
    "mass_matrix = torch.eye(2).cuda()\n",
    "mom = torch.tensor([0.1,0.1], requires_grad=True).cuda()\n",
    "cache = {}\n",
    "pos_samples, mom_samples, ratio = sampler.get_samples(init, 0.05, 5, 2, mass = mass_matrix, mom = mom)\n",
    "#get_samples(self, pos, dt, n_step_per_sample, n_sample, mass, mom=None):\n",
    "pos_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: tensor([ 0.,  0.])\n",
      "grad: tensor([ 0.,  0.])\n",
      "loop grad: tensor([ 0.,  0.], device='cuda:0')\n",
      "loop grad: tensor([ 0.,  0.], device='cuda:0')\n",
      "loop grad: tensor([ 0.,  0.], device='cuda:0')\n",
      "loop grad: tensor([ 0.,  0.], device='cuda:0')\n",
      "mom: tensor([-0.4941, -1.3525])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-02 *\n",
       "       [[ 0.0000,  0.0000],\n",
       "        [ 2.5000,  2.5000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = torch.zeros(2, requires_grad=True).cuda()\n",
    "mass_matrix = torch.eye(2).cuda()\n",
    "mom = torch.tensor([0.1,0.1], requires_grad=True).cuda()\n",
    "cache = {}\n",
    "pos_samples, mom_samples, ratio = sampler.get_samples(init, 0.05, 5, 2, mass = mass_matrix, mom = mom)\n",
    "#get_samples(self, pos, dt, n_step_per_sample, n_sample, mass, mom=None):\n",
    "pos_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    " def gradient(pos, cache):\n",
    "    #pos.retain_grad()\n",
    "    x = energy(pos, cache)\n",
    "    print(\"x: \"+str(x))\n",
    "    #x.backward(retain_graph=True)\n",
    "    x.backward()\n",
    "    g = pos.grad.clone()\n",
    "    #pos.grad.zero_()\n",
    "    #print(\"g:\"+str(g))\n",
    "    #pos.grad.data.zero_()\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    " def gradient(pos, cache):\n",
    "    #print(\"------calling energy function------\")\n",
    "    pos.retain_grad()\n",
    "    x = energy(pos, cache)\n",
    "    print(\"x: \"+str(x))\n",
    "    x.backward(retain_graph=True)\n",
    "\n",
    "    g = pos.grad.clone()\n",
    "    #pos.grad.zero_()\n",
    "    #print(\"g:\"+str(g))\n",
    "    pos.grad.data.zero_()\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([ 1.8379], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = torch.zeros(2, requires_grad=True).cuda()\n",
    "gradient(pos, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043755842097883399"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(z1_list)\n",
    "#plt.hist(z1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(z, cache):\n",
    "    #z.retain_grad()\n",
    "    z_cuda = z.cuda()\n",
    "    mean = 3*torch.ones([1,1], requires_grad=True).cuda()\n",
    "    cov_diag = torch.ones([1,1], requires_grad=True).cuda()\n",
    "    res = -multivariate_normal_diagonal_logpdf(mean, cov_diag, z_cuda)\n",
    "    #print(\"energy: \"+str(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 1\n",
      "Sample: 2\n",
      "Sample: 3\n",
      "Sample: 4\n",
      "Sample: 5\n",
      "Sample: 6\n",
      "Sample: 7\n",
      "Sample: 8\n",
      "Sample: 9\n",
      "Sample: 10\n",
      "Sample: 11\n",
      "Sample: 12\n",
      "Sample: 13\n",
      "Sample: 14\n",
      "Sample: 15\n",
      "Sample: 16\n",
      "Sample: 17\n",
      "Sample: 18\n",
      "Sample: 19\n",
      "Sample: 20\n",
      "Sample: 21\n",
      "Sample: 22\n",
      "Sample: 23\n",
      "Sample: 24\n",
      "Sample: 25\n",
      "Sample: 26\n",
      "Sample: 27\n",
      "Sample: 28\n",
      "Sample: 29\n",
      "Sample: 30\n",
      "Sample: 31\n",
      "Sample: 32\n",
      "Sample: 33\n",
      "Sample: 34\n",
      "Sample: 35\n",
      "Sample: 36\n",
      "Sample: 37\n",
      "Sample: 38\n",
      "Sample: 39\n",
      "Sample: 40\n",
      "Sample: 41\n",
      "Sample: 42\n",
      "Sample: 43\n",
      "Sample: 44\n",
      "Sample: 45\n",
      "Sample: 46\n",
      "Sample: 47\n",
      "Sample: 48\n",
      "Sample: 49\n",
      "Sample: 50\n",
      "Sample: 51\n",
      "Sample: 52\n",
      "Sample: 53\n",
      "Sample: 54\n",
      "Sample: 55\n",
      "Sample: 56\n",
      "Sample: 57\n",
      "Sample: 58\n",
      "Sample: 59\n",
      "Sample: 60\n",
      "Sample: 61\n",
      "Sample: 62\n",
      "Sample: 63\n",
      "Sample: 64\n",
      "Sample: 65\n",
      "Sample: 66\n",
      "Sample: 67\n",
      "Sample: 68\n",
      "Sample: 69\n",
      "Sample: 70\n",
      "Sample: 71\n",
      "Sample: 72\n",
      "Sample: 73\n",
      "Sample: 74\n",
      "Sample: 75\n",
      "Sample: 76\n",
      "Sample: 77\n",
      "Sample: 78\n",
      "Sample: 79\n",
      "Sample: 80\n",
      "Sample: 81\n",
      "Sample: 82\n",
      "Sample: 83\n",
      "Sample: 84\n",
      "Sample: 85\n",
      "Sample: 86\n",
      "Sample: 87\n",
      "Sample: 88\n",
      "Sample: 89\n",
      "Sample: 90\n",
      "Sample: 91\n",
      "Sample: 92\n",
      "Sample: 93\n",
      "Sample: 94\n",
      "Sample: 95\n",
      "Sample: 96\n",
      "Sample: 97\n",
      "Sample: 98\n",
      "Sample: 99\n",
      "Sample: 100\n",
      "Sample: 101\n",
      "Sample: 102\n",
      "Sample: 103\n",
      "Sample: 104\n",
      "Sample: 105\n",
      "Sample: 106\n",
      "Sample: 107\n",
      "Sample: 108\n",
      "Sample: 109\n",
      "Sample: 110\n",
      "Sample: 111\n",
      "Sample: 112\n",
      "Sample: 113\n",
      "Sample: 114\n",
      "Sample: 115\n",
      "Sample: 116\n",
      "Sample: 117\n",
      "Sample: 118\n",
      "Sample: 119\n",
      "Sample: 120\n",
      "Sample: 121\n",
      "Sample: 122\n",
      "Sample: 123\n",
      "Sample: 124\n",
      "Sample: 125\n",
      "Sample: 126\n",
      "Sample: 127\n",
      "Sample: 128\n",
      "Sample: 129\n",
      "Sample: 130\n",
      "Sample: 131\n",
      "Sample: 132\n",
      "Sample: 133\n",
      "Sample: 134\n",
      "Sample: 135\n",
      "Sample: 136\n",
      "Sample: 137\n",
      "Sample: 138\n",
      "Sample: 139\n",
      "Sample: 140\n",
      "Sample: 141\n",
      "Sample: 142\n",
      "Sample: 143\n",
      "Sample: 144\n",
      "Sample: 145\n",
      "Sample: 146\n",
      "Sample: 147\n",
      "Sample: 148\n",
      "Sample: 149\n",
      "Sample: 150\n",
      "Sample: 151\n",
      "Sample: 152\n",
      "Sample: 153\n",
      "Sample: 154\n",
      "Sample: 155\n",
      "Sample: 156\n",
      "Sample: 157\n",
      "Sample: 158\n",
      "Sample: 159\n",
      "Sample: 160\n",
      "Sample: 161\n",
      "Sample: 162\n",
      "Sample: 163\n",
      "Sample: 164\n",
      "Sample: 165\n",
      "Sample: 166\n",
      "Sample: 167\n",
      "Sample: 168\n",
      "Sample: 169\n",
      "Sample: 170\n",
      "Sample: 171\n",
      "Sample: 172\n",
      "Sample: 173\n",
      "Sample: 174\n",
      "Sample: 175\n",
      "Sample: 176\n",
      "Sample: 177\n",
      "Sample: 178\n",
      "Sample: 179\n",
      "Sample: 180\n",
      "Sample: 181\n",
      "Sample: 182\n",
      "Sample: 183\n",
      "Sample: 184\n",
      "Sample: 185\n",
      "Sample: 186\n",
      "Sample: 187\n",
      "Sample: 188\n",
      "Sample: 189\n",
      "Sample: 190\n",
      "Sample: 191\n",
      "Sample: 192\n",
      "Sample: 193\n",
      "Sample: 194\n",
      "Sample: 195\n",
      "Sample: 196\n",
      "Sample: 197\n",
      "Sample: 198\n",
      "Sample: 199\n",
      "Sample: 200\n",
      "Sample: 201\n",
      "Sample: 202\n",
      "Sample: 203\n",
      "Sample: 204\n",
      "Sample: 205\n",
      "Sample: 206\n",
      "Sample: 207\n",
      "Sample: 208\n",
      "Sample: 209\n",
      "Sample: 210\n",
      "Sample: 211\n",
      "Sample: 212\n",
      "Sample: 213\n",
      "Sample: 214\n",
      "Sample: 215\n",
      "Sample: 216\n",
      "Sample: 217\n",
      "Sample: 218\n",
      "Sample: 219\n",
      "Sample: 220\n",
      "Sample: 221\n",
      "Sample: 222\n",
      "Sample: 223\n",
      "Sample: 224\n",
      "Sample: 225\n",
      "Sample: 226\n",
      "Sample: 227\n",
      "Sample: 228\n",
      "Sample: 229\n",
      "Sample: 230\n",
      "Sample: 231\n",
      "Sample: 232\n",
      "Sample: 233\n",
      "Sample: 234\n",
      "Sample: 235\n",
      "Sample: 236\n",
      "Sample: 237\n",
      "Sample: 238\n",
      "Sample: 239\n",
      "Sample: 240\n",
      "Sample: 241\n",
      "Sample: 242\n",
      "Sample: 243\n",
      "Sample: 244\n",
      "Sample: 245\n",
      "Sample: 246\n",
      "Sample: 247\n",
      "Sample: 248\n",
      "Sample: 249\n",
      "Sample: 250\n",
      "Sample: 251\n",
      "Sample: 252\n",
      "Sample: 253\n",
      "Sample: 254\n",
      "Sample: 255\n",
      "Sample: 256\n",
      "Sample: 257\n",
      "Sample: 258\n",
      "Sample: 259\n",
      "Sample: 260\n",
      "Sample: 261\n",
      "Sample: 262\n",
      "Sample: 263\n",
      "Sample: 264\n",
      "Sample: 265\n",
      "Sample: 266\n",
      "Sample: 267\n",
      "Sample: 268\n",
      "Sample: 269\n",
      "Sample: 270\n",
      "Sample: 271\n",
      "Sample: 272\n",
      "Sample: 273\n",
      "Sample: 274\n",
      "Sample: 275\n",
      "Sample: 276\n",
      "Sample: 277\n",
      "Sample: 278\n",
      "Sample: 279\n",
      "Sample: 280\n",
      "Sample: 281\n",
      "Sample: 282\n",
      "Sample: 283\n",
      "Sample: 284\n",
      "Sample: 285\n",
      "Sample: 286\n",
      "Sample: 287\n",
      "Sample: 288\n",
      "Sample: 289\n",
      "Sample: 290\n",
      "Sample: 291\n",
      "Sample: 292\n",
      "Sample: 293\n",
      "Sample: 294\n",
      "Sample: 295\n",
      "Sample: 296\n",
      "Sample: 297\n",
      "Sample: 298\n",
      "Sample: 299\n",
      "Sample: 300\n",
      "Sample: 301\n",
      "Sample: 302\n",
      "Sample: 303\n",
      "Sample: 304\n",
      "Sample: 305\n",
      "Sample: 306\n",
      "Sample: 307\n",
      "Sample: 308\n",
      "Sample: 309\n",
      "Sample: 310\n",
      "Sample: 311\n",
      "Sample: 312\n",
      "Sample: 313\n",
      "Sample: 314\n",
      "Sample: 315\n",
      "Sample: 316\n",
      "Sample: 317\n",
      "Sample: 318\n",
      "Sample: 319\n",
      "Sample: 320\n",
      "Sample: 321\n",
      "Sample: 322\n",
      "Sample: 323\n",
      "Sample: 324\n",
      "Sample: 325\n",
      "Sample: 326\n",
      "Sample: 327\n",
      "Sample: 328\n",
      "Sample: 329\n",
      "Sample: 330\n",
      "Sample: 331\n",
      "Sample: 332\n",
      "Sample: 333\n",
      "Sample: 334\n",
      "Sample: 335\n",
      "Sample: 336\n",
      "Sample: 337\n",
      "Sample: 338\n",
      "Sample: 339\n",
      "Sample: 340\n",
      "Sample: 341\n",
      "Sample: 342\n",
      "Sample: 343\n",
      "Sample: 344\n",
      "Sample: 345\n",
      "Sample: 346\n",
      "Sample: 347\n",
      "Sample: 348\n",
      "Sample: 349\n",
      "Sample: 350\n",
      "Sample: 351\n",
      "Sample: 352\n",
      "Sample: 353\n",
      "Sample: 354\n",
      "Sample: 355\n",
      "Sample: 356\n",
      "Sample: 357\n",
      "Sample: 358\n",
      "Sample: 359\n",
      "Sample: 360\n",
      "Sample: 361\n",
      "Sample: 362\n",
      "Sample: 363\n",
      "Sample: 364\n",
      "Sample: 365\n",
      "Sample: 366\n",
      "Sample: 367\n",
      "Sample: 368\n",
      "Sample: 369\n",
      "Sample: 370\n",
      "Sample: 371\n",
      "Sample: 372\n",
      "Sample: 373\n",
      "Sample: 374\n",
      "Sample: 375\n",
      "Sample: 376\n",
      "Sample: 377\n",
      "Sample: 378\n",
      "Sample: 379\n",
      "Sample: 380\n",
      "Sample: 381\n",
      "Sample: 382\n",
      "Sample: 383\n",
      "Sample: 384\n",
      "Sample: 385\n",
      "Sample: 386\n",
      "Sample: 387\n",
      "Sample: 388\n",
      "Sample: 389\n",
      "Sample: 390\n",
      "Sample: 391\n",
      "Sample: 392\n",
      "Sample: 393\n",
      "Sample: 394\n",
      "Sample: 395\n",
      "Sample: 396\n",
      "Sample: 397\n",
      "Sample: 398\n",
      "Sample: 399\n",
      "Sample: 400\n",
      "Sample: 401\n",
      "Sample: 402\n",
      "Sample: 403\n",
      "Sample: 404\n",
      "Sample: 405\n",
      "Sample: 406\n",
      "Sample: 407\n",
      "Sample: 408\n",
      "Sample: 409\n",
      "Sample: 410\n",
      "Sample: 411\n",
      "Sample: 412\n",
      "Sample: 413\n",
      "Sample: 414\n",
      "Sample: 415\n",
      "Sample: 416\n",
      "Sample: 417\n",
      "Sample: 418\n",
      "Sample: 419\n",
      "Sample: 420\n",
      "Sample: 421\n",
      "Sample: 422\n",
      "Sample: 423\n",
      "Sample: 424\n",
      "Sample: 425\n",
      "Sample: 426\n",
      "Sample: 427\n",
      "Sample: 428\n",
      "Sample: 429\n",
      "Sample: 430\n",
      "Sample: 431\n",
      "Sample: 432\n",
      "Sample: 433\n",
      "Sample: 434\n",
      "Sample: 435\n",
      "Sample: 436\n",
      "Sample: 437\n",
      "Sample: 438\n",
      "Sample: 439\n",
      "Sample: 440\n",
      "Sample: 441\n",
      "Sample: 442\n",
      "Sample: 443\n",
      "Sample: 444\n",
      "Sample: 445\n",
      "Sample: 446\n",
      "Sample: 447\n",
      "Sample: 448\n",
      "Sample: 449\n",
      "Sample: 450\n",
      "Sample: 451\n",
      "Sample: 452\n",
      "Sample: 453\n",
      "Sample: 454\n",
      "Sample: 455\n",
      "Sample: 456\n",
      "Sample: 457\n",
      "Sample: 458\n",
      "Sample: 459\n",
      "Sample: 460\n",
      "Sample: 461\n",
      "Sample: 462\n",
      "Sample: 463\n",
      "Sample: 464\n",
      "Sample: 465\n",
      "Sample: 466\n",
      "Sample: 467\n",
      "Sample: 468\n",
      "Sample: 469\n",
      "Sample: 470\n",
      "Sample: 471\n",
      "Sample: 472\n",
      "Sample: 473\n",
      "Sample: 474\n",
      "Sample: 475\n",
      "Sample: 476\n",
      "Sample: 477\n",
      "Sample: 478\n",
      "Sample: 479\n",
      "Sample: 480\n",
      "Sample: 481\n",
      "Sample: 482\n",
      "Sample: 483\n",
      "Sample: 484\n",
      "Sample: 485\n",
      "Sample: 486\n",
      "Sample: 487\n",
      "Sample: 488\n",
      "Sample: 489\n",
      "Sample: 490\n",
      "Sample: 491\n",
      "Sample: 492\n",
      "Sample: 493\n",
      "Sample: 494\n",
      "Sample: 495\n",
      "Sample: 496\n",
      "Sample: 497\n",
      "Sample: 498\n",
      "Sample: 499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(100):\\n    z1_list.append(pos_samples[i].item())\\n#z1_list.append(pos_samples[1][0].item())\\n    #z2_list.append(pos_samples[1][1].item())\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = IsotropicHmcSampler(energy, energy_grad=None, prng=None, mom_resample_coeff=0., dtype=np.float64)\n",
    "\n",
    "z1_list = []\n",
    "#z2_list = []\n",
    "m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "\n",
    "init = torch.zeros(1, requires_grad=True).cuda()\n",
    "mass_matrix = torch.tensor([1.]).cuda()\n",
    "pos_samples, mom_samples, ratio = sampler.get_samples(init, 0.01, 5, 1, mass_matrix)\n",
    "for i in range(500):\n",
    "    z1_list.append(pos_samples[i].item())\n",
    "    \n",
    "\"\"\"\n",
    "for i in range(500):\n",
    "    init = torch.ones(1, requires_grad=True).cuda()\n",
    "    mass_matrix = torch.tensor([1.]).cuda()\n",
    "    mom = m.sample().cuda()\n",
    "    pos_samples, mom_samples, ratio = sampler.get_samples(init, 0.01, 10, 2, mass_matrix,mom)\n",
    "    z1_list.append(pos_samples[1][0].item())\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "for i in range(100):\n",
    "    z1_list.append(pos_samples[i].item())\n",
    "#z1_list.append(pos_samples[1][0].item())\n",
    "    #z2_list.append(pos_samples[1][1].item())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1149381328467278"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19fa0134dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.var(z1_list)\n",
    "plt.hist(z1_list)\n",
    "np.mean(z1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h = plt.hist2d(z1_list, z2_list, bins = [100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-15.3818,  -3.7568], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.tensor([[0.,0.,0.],[0.,0.,0.]]).cuda()\n",
    "cov_diag = torch.tensor([[1.,1.,1.],[1.,1.,1.]]).cuda()\n",
    "x = torch.tensor([[5.,0.5,0.],[1.,1.,0.]]).cuda()\n",
    "multivariate_normal_diagonal_logpdf(mean, cov_diag, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  1.]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean.shape\n",
    "torch.diag(cov_diag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(y,cache):\n",
    "    mean = \n",
    "    multivariate_normal_diagonal_logpdf(mean, cov_diag, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.tensor([[1.,0.,0.],[0.,0.,0.]], requires_grad=True)\n",
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = torch.sum(mean, dim=1)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-8a43000430b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "res.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
