{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('F:\\Cambridge\\Project\\MHMC-for-VAE\\change_of_variable')\n",
    "sys.path.append('F:\\Cambridge\\Project\\MHMC-for-VAE\\hmc_pytorch')\n",
    "from change_of_variable_pytorch import * \n",
    "from hmc_base_pytorch import *\n",
    "from hmc_unconstrained_pytorch import *\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.normal import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import time\n",
    "import torchvision\n",
    "\n",
    "\n",
    "cuda = True\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "seed = 1\n",
    "z_dim = 20\n",
    "\n",
    "# Data preparation\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "train_data = datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.MNIST('../data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "def binarization(data):\n",
    "    s = np.random.uniform(size = data.shape)\n",
    "    out = np.array(s<data).astype(float)\n",
    "    return out\n",
    "\n",
    "result = []\n",
    "for batch_idx, (data, _) in enumerate(train_loader):\n",
    "    data = data.view(-1, 784).numpy()\n",
    "    bi_data = binarization(data)\n",
    "    d = torch.from_numpy(bi_data)\n",
    "    result.append(d)\n",
    "    \n",
    "result_test = []\n",
    "for batch_idx, (data, _) in enumerate(test_loader):\n",
    "    data = data.view(-1, 784).numpy()\n",
    "    bi_data = binarization(data)\n",
    "    d = torch.from_numpy(bi_data)\n",
    "    result_test.append(d)\n",
    "\n",
    "############################################################\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5*logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    #return eps.mul(std).add_(mu)\n",
    "    return eps*std+mu\n",
    "\n",
    "def log_prior(z):\n",
    "    dim = z.shape[1]\n",
    "    mean = torch.zeros(dim).cuda()\n",
    "    cov = torch.eye(dim).cuda()\n",
    "    m = MultivariateNormal(mean, cov)\n",
    "    m.requires_grad=True\n",
    "    return m.log_prob(z)\n",
    "\n",
    "def multivariate_normal_diagonal_logpdf(mean, cov_diag, x):\n",
    "    n = x.shape[0] # number of samples\n",
    "    k = x.shape[1] # dimension\n",
    "    t1 = -0.5*(x - mean)*(1/cov_diag)*(x-mean)\n",
    "    t1 = torch.sum(t1, dim=1)\n",
    "    t2 = torch.ones(n).cuda()*0.5*k*torch.log(torch.tensor([2*math.pi]).cuda()) + 0.5*torch.sum(torch.log(cov_diag),dim=1)\n",
    "    return t1 - t2\n",
    "############################################################\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, 784)\n",
    "    # single hidden layer\n",
    "    def forward(self, x):\n",
    "        #x = x.view(-1, 784)\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return F.sigmoid(self.fc2(h1))\n",
    "    \n",
    "class q_z0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(q_z0, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 300)\n",
    "        self.fc2 = nn.Linear(300, 300)\n",
    "        self.fc31 = nn.Linear(300, z_dim)\n",
    "        self.fc32 = nn.Linear(300, z_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        h1 = F.softplus(self.fc1(x))\n",
    "        h2 = F.softplus(self.fc2(h1))\n",
    "        logvar = self.fc31(h2)\n",
    "        mu = self.fc32(h2)\n",
    "        return mu, logvar\n",
    "    \n",
    "class r_v(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(r_v, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim + 784, 300)\n",
    "        self.fc21 = nn.Linear(300, z_dim)\n",
    "        self.fc22 = nn.Linear(300, z_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784 + z_dim)\n",
    "        h1 = F.softplus(self.fc1(x))\n",
    "        logvar = self.fc21(h1)\n",
    "        mu = self.fc22(h1)\n",
    "        return mu, logvar\n",
    "\n",
    "class q_v(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(q_v, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 300)\n",
    "        # no need to output mu because the mean of momentum is default 0\n",
    "        self.fc21 = nn.Linear(300, z_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        h1 = F.softplus(self.fc1(x))\n",
    "        logvar = self.fc21(h1)\n",
    "        return logvar\n",
    "############################################################\n",
    "decoder = decoder().to(device)\n",
    "q_z0 = q_z0().to(device)\n",
    "r_v = r_v().to(device)\n",
    "q_v = q_v().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO(data, decoder, q_z0, r_v, q_v, log_step, T):#q_z0_mean, q_z0_logvar):\n",
    "    \n",
    "    #file1 = open(\"q_z0_mean\"+str(epoch)+\".txt\",\"w\")\n",
    "    #file1 = open(\"q_z0_var\"+str(epoch)+\".txt\",\"w\")\n",
    "    #file1 = open(\"z0\"+str(epoch)+\".txt\",\"w\")\n",
    "    batch_size = data.view(-1, 784).shape[0]\n",
    "    data = data.to(device)\n",
    "    \n",
    "    q_z0_mean, q_z0_logvar = q_z0(data)\n",
    "    # sample z0\n",
    "    z0 = reparameterize(q_z0_mean, q_z0_logvar)\n",
    "    \n",
    "    # compute q(z0|x)\n",
    "    var_z0 = torch.exp(q_z0_logvar)\n",
    "    log_q_z0 = multivariate_normal_diagonal_logpdf(q_z0_mean, var_z0, z0)\n",
    "    \n",
    "    #print(\"q z0 mean: \"+str(q_z0_mean))\n",
    "    #print(\"q z0 var: \"+str(var_z0))\n",
    "    #print(\"z0: \"+str(z0))\n",
    "    #print(\"q(z0|x): \"+str(log_q_z0))\n",
    "    #print(\"np q(z0|x): \"+str(np_log_q_z0))\n",
    "    \n",
    "    # compute prior\n",
    "    log_prior_z0 = log_prior(z0)\n",
    "    \n",
    "    \n",
    "    # compute joint\n",
    "    decoder_output = decoder(z0)\n",
    "    log_likelihood = 0. - F.binary_cross_entropy(decoder_output, data.view(-1, 784).float(), size_average=False, reduce=False)\n",
    "    log_likelihood = torch.sum(log_likelihood, dim = 1)\n",
    "    log_joint = log_prior_z0 + log_likelihood\n",
    "    \n",
    "    \n",
    "    # initial L\n",
    "    #L = log_joint - log_q_z0\n",
    "    L = -log_q_z0\n",
    "    #print(\"initial L: \"+str(L))\n",
    "    #print(\"log likelihood 0: \"+str(torch.mean(log_likelihood)))\n",
    "    \n",
    "    for i in range(T):\n",
    "        # sample v1\n",
    "        \n",
    "        logvar_v1 = q_v(data)\n",
    "        \n",
    "        #logvar_v1 = torch.zeros([batch_size, z_dim], requires_grad = True).cuda()\n",
    "        var_v1 = torch.exp(logvar_v1)\n",
    "        mu_v1 = torch.zeros([logvar_v1.shape[0], logvar_v1.shape[1]],requires_grad = True).cuda()\n",
    "        v1 = reparameterize(mu_v1, logvar_v1)\n",
    "        #print(\"v1: \"+str(v1.shape))\n",
    "        \n",
    "        # compute q(v1|x) \n",
    "        log_q_v1 = multivariate_normal_diagonal_logpdf(mu_v1, var_v1 ,v1)\n",
    "        #print(\"log_q_v1: \"+str(log_q_v1.shape))\n",
    "        #print(\"var_v1: \"+str(var_v1))\n",
    "        \n",
    "        mass_diag = var_v1\n",
    "        #print(\"mass_diag: \"+str(mass_diag))\n",
    "        \n",
    "        def energy_function(z, cache):\n",
    "            z = z.view(batch_size, z_dim)\n",
    "            z_cuda = z.cuda()\n",
    "            all_log_prior = log_prior(z_cuda)\n",
    "            #print(\"all prior: \"+str(all_log_prior.shape))\n",
    "            all_log_prior = torch.sum(all_log_prior)\n",
    "            #print(all_log_prior.shape)\n",
    "            decoder_output = decoder(z_cuda)\n",
    "\n",
    "            all_log_likelihood = 0. - F.binary_cross_entropy(decoder_output, data.view(-1, 784).float(), size_average=False)\n",
    "            #print(\"one_log_likelihood: \"+str(one_log_likelihood.shape))\n",
    "            all_log_joint = all_log_prior + all_log_likelihood\n",
    "            return 0 - all_log_joint\n",
    "        \n",
    "        \n",
    "        init = z0.view(batch_size*z_dim)\n",
    "        mass_diag = mass_diag.view(mass_diag.shape[0]*mass_diag.shape[1])\n",
    "        mass_matrix = torch.diag(mass_diag)\n",
    "        mom = v1.view(v1.shape[0]*v1.shape[1])\n",
    "        step = torch.exp(log_step)\n",
    "        #print(\"init: \"+str(init.shape))\n",
    "        #print(\"mass: \"+str(mass_matrix.shape))\n",
    "        \n",
    "        # clip step size\n",
    "        low = 0.0005\n",
    "        high = 0.5\n",
    "        torch.clamp(step, min=low, max=high)\n",
    "        step = step.repeat(1, batch_size)\n",
    "        step = step.view(step.shape[1])\n",
    "        step_cuda = step.cuda()\n",
    "        print(\"step: \"+str(step_cuda.shape))\n",
    "        \n",
    "        sampler = IsotropicHmcSampler(energy_function, energy_grad=None, prng=None, mom_resample_coeff=0., dtype=np.float64)\n",
    "        #pos_samples, mom_samples, ratio = sampler.get_samples(init, 0.0001, 5, 2, mass_matrix, mom = mom)\n",
    "        pos_samples, mom_samples, ratio = sampler.get_samples(init, step_cuda, 5, 2, mass_matrix, mom = mom)\n",
    "        \n",
    "        zt = pos_samples[1].cuda()\n",
    "        vt = mom_samples[1].cuda()\n",
    "        \n",
    "        zt = zt.view(batch_size, z_dim)\n",
    "        vt = vt.view(batch_size, z_dim)\n",
    "\n",
    "        # get joint probaility p(x, zt)\n",
    "        log_prior_zt = log_prior(zt)\n",
    "        decoder_output_t = decoder(zt)\n",
    "        #print(\"log prior: \" + str(log_prior_zt.shape))\n",
    "        #print(\"decoder: \" + str(decoder_output_t.shape))\n",
    "\n",
    "        log_likelihood_t = 0. - F.binary_cross_entropy(decoder_output_t, data.view(-1, 784).float(), size_average=False, reduce=False)\n",
    "        log_likelihood_t = torch.sum(log_likelihood_t, dim = 1)\n",
    "        #print(\"log ll: \" + str(log_likelihood_t.shape))\n",
    "        \n",
    "        log_joint_t = log_prior_zt + log_likelihood_t\n",
    "        #print(\"log joint t: \" + str(log_joint_t.shape))\n",
    "        \n",
    "        # get r(vt|x,zt)\n",
    "        d = data.view(-1, 784)\n",
    "        #print(d.shape)\n",
    "        new_data = torch.cat((d.float(), zt), 1) # append data with zt\n",
    "        #print(\"new data: \"+str(one_new_data.shape))\n",
    "\n",
    "        mu_vt, logvar_vt = r_v(new_data)\n",
    "        var_vt = torch.exp(logvar_vt)\n",
    "        #print(\"var_vt: \"+str(var_vt.shape))\n",
    "\n",
    "        log_r_vt = multivariate_normal_diagonal_logpdf(mu_vt, var_vt, vt)\n",
    "        \"\"\"\n",
    "        print(\"log prior t: \"+str(torch.mean(log_prior_zt)))\n",
    "        print(\"log likelihood t: \"+str(torch.mean(log_likelihood_t)))\n",
    "        print(\"joint t: \"+str(torch.mean(log_joint_t)))\n",
    "        print(\"reverse: \"+str(torch.mean(log_r_vt)))\n",
    "        #print(log_joint[j])\n",
    "        print(\"forward: \"+str(torch.mean(log_q_v1)))\n",
    "        print(\"q(z0|x): \"+str(torch.mean(log_q_z0)))\n",
    "        print(\"========================================`\")\n",
    "        \"\"\"\n",
    "        #file1 = open(\"q_z0_mean\"+str(epoch)+\".txt\",\"w\")\n",
    "        #file2 = open(\"q_z0_var\"+str(epoch)+\".txt\",\"w\")\n",
    "        #file3 = open(\"z0\"+str(epoch)+\".txt\",\"w\")\n",
    "        \n",
    "        # get L for each sample\n",
    "        #one_log_alpha = one_log_joint_t + one_log_r_vt - log_joint[j] - log_q_v1[j]\n",
    "        log_alpha = log_joint_t + log_r_vt - log_q_v1\n",
    "        #one_log_alpha = log_joint[j] #+ one_log_r_vt - log_q_v1[j]\n",
    "\n",
    "        L = L + log_alpha\n",
    "        #print(\"L: \"+str(L.shape))\n",
    "    \n",
    "    \n",
    "    return torch.mean(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "z_dim = 20\n",
    "\n",
    "params1 = list(decoder.parameters())+list(q_z0.parameters())+list(r_v.parameters())+list(q_v.parameters())\n",
    "optimizer1 = optim.Adam(params1, lr=0.0035)#, weight_decay=1e-4)\n",
    "#optimizer1 = optim.Adam([q_z0_mean], lr=0.005)\n",
    "log_step = 0.005*torch.ones(z_dim, requires_grad=True)\n",
    "optimizer2 = optim.Adam([log_step], lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "    print(\"Epoch: \"+str(epoch+1))\n",
    "    file = open(\"result11_\"+str(epoch)+\".txt\",\"w\")\n",
    "    file_test = open(\"result11_test_\"+str(epoch)+\".txt\",\"w\")\n",
    "    for i in range(len(result)):\n",
    "        print(\"++++++++++\"+\" Epoch: \"+str(epoch+1)+\" batch: \" + str(i) + \" ++++++++++\")\n",
    "        data = result[i].float()\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        loss = 0. - ELBO(data, decoder, q_z0, r_v, q_v, log_step, 1)#_mean_cuda, q_z0_logvar_cuda)\n",
    "        print(\"ELBO: \"+str(0-loss.item()))\n",
    "        loss.backward()\n",
    "        #nn.utils.clip_grad_norm_(q_z0.parameters(), 1)\n",
    "        #print(q_z0.fc1.weight.grad)\n",
    "        #print(q_z0.fc31.weight.grad)\n",
    "        #print(q_z0.fc32.weight.grad)\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        file.write(str(0.-loss.item())+\"\\n\")\n",
    "    file.close()\n",
    "    for i in range(len(result_test)):\n",
    "        print(\"++++++++++ test batch: \" + str(i) + \" ++++++++++\")\n",
    "        data = result_test[i].float()\n",
    "        loss = 0. - ELBO(data, decoder, q_z0, r_v, q_v, log_step, 1)\n",
    "        print(\"ELBO: \"+str(0-loss.item()))\n",
    "        file_test.write(str(0.-loss.item())+\"\\n\")\n",
    "        \n",
    "    file_test.close()\n",
    "    \n",
    "    sample = torch.randn(64, 20).to(device)\n",
    "    sample = decoder(sample).cpu()\n",
    "    save_image(sample.view(64, 1, 28, 28), 'sample11_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = torch.tensor([1., 2.])\n",
    "tmp = torch.tensor([2., 4.])\n",
    "x = dt*tmp\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
