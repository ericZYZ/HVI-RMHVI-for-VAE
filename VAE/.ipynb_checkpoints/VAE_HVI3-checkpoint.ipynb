{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('F:\\Cambridge\\Project\\MHMC-for-VAE\\change_of_variable')\n",
    "sys.path.append('F:\\Cambridge\\Project\\MHMC-for-VAE\\hmc_pytorch')\n",
    "from change_of_variable_pytorch import * \n",
    "from hmc_base_pytorch import *\n",
    "from hmc_unconstrained_pytorch import *\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.normal import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import time\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "z_dim = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "train_data = datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.MNIST('../data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarization(data):\n",
    "    s = np.random.uniform(size = data.shape)\n",
    "    out = np.array(s<data).astype(float)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for batch_idx, (data, _) in enumerate(train_loader):\n",
    "    data = data.view(-1, 784).numpy()\n",
    "    bi_data = binarization(data)\n",
    "    d = torch.from_numpy(bi_data)\n",
    "    result.append(d)\n",
    "    #result.append(binarization(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.train_data[0].numpy().shape\n",
    "result[0][0].reshape(28,28).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAC/ZJREFUeJzt3UHIZeV9x/Hvr2oIqNARcZgarWmRdJGFKWIXDWG6SGqz0SwskS4mZDFZ1JLsItkoJKGhtGmhhYIhkik0BsEYxZYYCUkNXYijhDg6MdpgzcRhBplFdFEk+u/iPRPejO/73jvvPfee+77/7wcu997jvef8OePvfZ5znnPuk6pCUj+/M3UBkqZh+KWmDL/UlOGXmjL8UlOGX2rK8EtNGX5tKckPk/xfkjeHx4tT16RxGX7t5K6qumJ4fGDqYjQuwy81Zfi1k79N8nqS/05yeOpiNK54bb+2kuRPgBeAt4BPAv8C3FRV/zNpYRqN4ddcknwX+I+q+uepa9E47PZrXgVk6iI0HsOvd0nyu0n+PMl7k1ya5K+AjwCPT12bxnPp1AVoLV0GfAn4I+Bt4KfA7VXlWP8+4jG/1JTdfqkpwy81Zfilpgy/1NRKz/Yn8eyitGRVNdf1GAu1/EluTfJikpeT3L3IuiSt1q6H+pJcAvwM+ChwCngauLOqXtjhO7b80pKtouW/BXi5qn5eVW8B3wJuW2B9klZokfBfC/xi0/tTw7LfkuRokuNJji+wLUkjW+SE31Zdi3d166vqPuA+sNsvrZNFWv5TwHWb3r8PeG2xciStyiLhfxq4Mcn7k7yHjR98eHScsiQt2667/VX16yR3sXGb5yXA/VX1/GiVSVqqld7V5zG/tHwruchH0t5l+KWmDL/UlOGXmjL8UlOGX2rK8EtNGX6pKcMvNWX4paYMv9SU4ZeaMvxSU4ZfasrwS00Zfqkpwy81Zfilpgy/1JThl5oy/FJThl9qyvBLTRl+qSnDLzVl+KWmDL/UlOGXmjL8UlOGX2rq0kW+nOQV4A3gbeDXVXXzGEVJWr6Fwj/4s6p6fYT1SFohu/1SU4uGv4DvJXkmydGtPpDkaJLjSY4vuC1JI0pV7f7Lye9V1WtJrgGeAP6mqp7c4fO735ikuVRV5vncQi1/Vb02PJ8FHgZuWWR9klZn1+FPcnmSK8+/Bj4GnBirMEnLtcjZ/oPAw0nOr+ebVfXdUaqStHQLHfNf9MY85peWbiXH/JL2LsMvNWX4paYMv9SU4ZeaGuPGHmkpFh2JGoahtQ1bfqkpwy81Zfilpgy/1JThl5oy/FJThl9qynH+Ecwaj3a8eWvLvqN0p/X7b2LLL7Vl+KWmDL/UlOGXmjL8UlOGX2rK8EtNOc6/Asu+L33BWZd2/d15rPLXoXVxbPmlpgy/1JThl5oy/FJThl9qyvBLTRl+qSnH+feAZY6V7+dxeO/Z39nMlj/J/UnOJjmxadlVSZ5I8tLwfGC5ZUoa2zzd/m8At16w7G7g+1V1I/D94b2kPWRm+KvqSeDcBYtvA44Nr48Bt49cl6Ql2+0x/8GqOg1QVaeTXLPdB5McBY7ucjuSlmTpJ/yq6j7gPoAk+/fskrTH7Hao70ySQwDD89nxSpK0CrsN/6PAkeH1EeCRccqRtCqZ4zfnHwAOA1cDZ4B7gO8ADwLXA68Cd1TVhScFt1pXy27/fh5Ln5Lj+Furqrl2zMzwj8nwa0yGf2vzht/Le6WmDL/UlOGXmjL8UlOGX2rKW3pXYJ3PSi86vfgyRzLWeb/tB7b8UlOGX2rK8EtNGX6pKcMvNWX4paYMv9SU4/zNLXss3bH69WXLLzVl+KWmDL/UlOGXmjL8UlOGX2rK8EtNOc6vHS16v/5O3/cagGnZ8ktNGX6pKcMvNWX4paYMv9SU4ZeaMvxSU47z73POEKztzGz5k9yf5GySE5uW3Zvkl0l+PDw+vtwyJY1tnm7/N4Bbt1j+j1V10/D4z3HLkrRsM8NfVU8C51ZQi6QVWuSE311JfjIcFhzY7kNJjiY5nuT4AtuSNLLMc0IoyQ3AY1X1weH9QeB1oIAvAoeq6tNzrMezTyu2zif8vLFnOapqrh27q5a/qs5U1dtV9Q7wNeCW3axH0nR2Ff4khza9/QRwYrvPSlpPM8f5kzwAHAauTnIKuAc4nOQmNrr9rwCfWWKNmmGdu/Y7WbTuWYcN/pbAzuY65h9tYx7zL8VeDf+iDP/WlnrML2nvM/xSU4ZfasrwS00Zfqkpb+ndBxY5c73skYIpz6rv5zP6Y7Dll5oy/FJThl9qyvBLTRl+qSnDLzVl+KWmHOff5/bzOL4WY8svNWX4paYMv9SU4ZeaMvxSU4ZfasrwS005zr/PLfILt9rfbPmlpgy/1JThl5oy/FJThl9qyvBLTRl+qamZ4U9yXZIfJDmZ5Pkknx2WX5XkiSQvDc8Hll+uLlZV7fiYJcmOD+1dM6foTnIIOFRVzya5EngGuB34FHCuqr6S5G7gQFV9fsa6vKJkxRa9iMeA7z2jTdFdVaer6tnh9RvASeBa4Dbg2PCxY2z8QZC0R1zUMX+SG4APAU8BB6vqNGz8gQCuGbs4Scsz97X9Sa4AHgI+V1W/mrc7mOQocHR35UlalpnH/ABJLgMeAx6vqq8Oy14EDlfV6eG8wA+r6gMz1uMx/4p5zN/PaMf82fjX/zpw8nzwB48CR4bXR4BHLrZISdOZ52z/h4EfAc8B7wyLv8DGcf+DwPXAq8AdVXVuxrps+aUlm7fln6vbPxbDLy3faN1+SfuT4ZeaMvxSU4ZfasrwS00Zfqkpf7q7uTmu81hRJVo1W36pKcMvNWX4paYMv9SU4ZeaMvxSU4Zfaspx/n3OcXxtx5ZfasrwS00Zfqkpwy81Zfilpgy/1JThl5oy/FJThl9qyvBLTRl+qSnDLzVl+KWmDL/UlOGXmpoZ/iTXJflBkpNJnk/y2WH5vUl+meTHw+Pjyy9X0lgyx489HAIOVdWzSa4EngFuB/4SeLOq/n7ujSU7b0yj88c8+qmquf5RZ/6ST1WdBk4Pr99IchK4drHyJE3too75k9wAfAh4alh0V5KfJLk/yYFtvnM0yfEkxxeqVNKoZnb7f/PB5Argv4AvV9W3kxwEXgcK+CIbhwafnrEOu/0rZre/n3m7/XOFP8llwGPA41X11S3++w3AY1X1wRnrMfwrZvj7mTf885ztD/B14OTm4A8nAs/7BHDiYouUNJ15zvZ/GPgR8BzwzrD4C8CdwE1sdPtfAT4znBzcaV22/Cs272HdduwZ7D2jdvvHYvhXz/D3M1q3X9L+ZPilpgy/1JThl5oy/FJThl9qyim69zmH6rQdW36pKcMvNWX4paYMv9SU4ZeaMvxSU4ZfamrV4/yvA/+76f3Vw7J1tK61rWtdYG27NWZtvz/vB1d6P/+7Np4cr6qbJytgB+ta27rWBda2W1PVZrdfasrwS01NHf77Jt7+Tta1tnWtC6xttyapbdJjfknTmbrllzQRwy81NUn4k9ya5MUkLye5e4oatpPklSTPDdOOTzq/4DAH4tkkJzYtuyrJE0leGp63nCNxotrWYtr2HaaVn3Tfrdt09ys/5k9yCfAz4KPAKeBp4M6qemGlhWwjySvAzVU1+QUhST4CvAn82/mp0JL8HXCuqr4y/OE8UFWfX5Pa7uUip21fUm3bTSv/KSbcd2NOdz+GKVr+W4CXq+rnVfUW8C3gtgnqWHtV9SRw7oLFtwHHhtfH2PifZ+W2qW0tVNXpqnp2eP0GcH5a+Un33Q51TWKK8F8L/GLT+1NMuAO2UMD3kjyT5OjUxWzh4Plp0Ybnayau50Izp21fpQumlV+bfbeb6e7HNkX4t/pRuXUab/zTqvpj4C+Avx66t5rPvwJ/yMYcjqeBf5iymGFa+YeAz1XVr6asZbMt6ppkv00R/lPAdZvevw94bYI6tlRVrw3PZ4GH2ThMWSdnzs+QPDyfnbie36iqM1X1dlW9A3yNCffdMK38Q8C/V9W3h8WT77ut6ppqv00R/qeBG5O8P8l7gE8Cj05Qx7skuXw4EUOSy4GPsX5Tjz8KHBleHwEembCW37Iu07ZvN608E++7dZvufpIr/IahjH8CLgHur6ovr7yILST5AzZae9i43fmbU9aW5AHgMBu3fJ4B7gG+AzwIXA+8CtxRVSs/8bZNbYe5yGnbl1TbdtPKP8WE+27M6e5HqcfLe6WevMJPasrwS00Zfqkpwy81Zfilpgy/1JThl5r6f2hz2RnOiavaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22ee9162048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot one example\n",
    "print(train_data.train_data.size())                 # (60000, 28, 28)\n",
    "print(train_data.train_labels.size())               # (60000)\n",
    "#plt.imshow(train_data.train_data[0].numpy(), cmap='gray')\n",
    "plt.imshow(result[0][0].reshape(28,28), cmap='gray')\n",
    "plt.title('%i' % train_data.train_labels[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5*logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(z):\n",
    "    dim = z.shape[1]\n",
    "    mean = torch.zeros(dim).cuda()\n",
    "    cov = torch.eye(dim).cuda()\n",
    "    m = MultivariateNormal(mean, cov)\n",
    "    m.requires_grad=True\n",
    "    return m.log_prob(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_normal_logpdf(mean, cov, x):\n",
    "    mean = mean.cuda()\n",
    "    cov = cov.cuda()\n",
    "    k = x.shape[0]\n",
    "    t1 = -0.5*(x - mean).view(1, k)@torch.inverse(cov)@(x - mean).view(k, 1)\n",
    "    t2 = 0.5*k*torch.log(2*torch.tensor([math.pi]).cuda()) + 0.5*torch.log(torch.det(cov))\n",
    "    return t1 - t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_normal_diagonal_logpdf(mean, cov_diag, x):\n",
    "    mean = mean.cuda()\n",
    "    cov_diag = cov_diag.cuda()\n",
    "    n = x.shape[0] # number of samples\n",
    "    k = x.shape[1] # dimension\n",
    "    t1 = -0.5*(x - mean)*(1/cov_diag)*(x-mean)\n",
    "    t1 = torch.sum(t1, dim=1)\n",
    "    t2 = 0.5*k*torch.log(2*torch.tensor([math.pi]).cuda()) + 0.5*torch.log(torch.prod(cov_diag,1)).cuda()\n",
    "    return t1 - t2\n",
    "    #return t1 - t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0000, -0.0000, -0.0000],\n",
      "        [-0.5000, -0.5000, -0.0000]], device='cuda:0')\n",
      "tensor([[ 2.,  0.,  0.],\n",
      "        [ 1.,  1.,  0.]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-4.7568, -3.7568], device='cuda:0')"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.tensor([[0.,0.,0.],[0.,0.,0.]]).cuda()\n",
    "cov_diag = torch.tensor([[1.,1.,1.],[1.,1.,1.]]).cuda()\n",
    "x = torch.tensor([[2.,0.,0.],[1.,1.,0.]]).cuda()\n",
    "multivariate_normal_diagonal_logpdf(mean, cov_diag, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.7568]], device='cuda:0')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.tensor([0.,0.,0.]).cuda()\n",
    "cov = torch.eye(3).cuda()\n",
    "x = torch.tensor([1.,1.,0.]).cuda()\n",
    "multivariate_normal_logpdf(mean, cov, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  1.,  4.])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_diag = torch.tensor([[1.,2.,1.],[1.,1.,1.],[4.,1.,1.]])\n",
    "torch.prod(cov_diag,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1.,1.,1.],[2.,1.,0.]])\n",
    "y = torch.tensor([[2.,0.,0.],[1.,1.,1.]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.tensor([[[0.,0.,0.]],[[0.,1.,0.]],[[0.,0.,0.]]])\n",
    "mean.shape\n",
    "cov = torch.tensor([[[1.,0.,0.],[0.,1.,0.],[0.,0.,1.]],[[1.,0.,0.],[0.,1.,0.],[0.,0.,1.]],[[1.,0.,0.],[0.,1.,0.],[0.,0.,1.]]])\n",
    "cov.shape\n",
    "x = torch.tensor([[[0., 2.,0.]],[[0., 0.,0.]],[[0., 0.,0.]]])\n",
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.tensor([[[0.,0.]],[[0.,1.]],[[0.,0.]]])\n",
    "mean.shape\n",
    "cov = torch.tensor([[[1.,0.],[0.,1.]],[[1.,0.],[0.,1.]],[[1.,0.],[0.,1.]]])\n",
    "cov.shape\n",
    "x = torch.tensor([[[0., 2.]],[[0., 0.]],[[0., 0.]]])\n",
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.8379, -3.8379, -3.8379],\n",
       "        [-2.3379, -2.3379, -2.3379],\n",
       "        [-1.8379, -1.8379, -1.8379]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = MultivariateNormal(mean, cov)\n",
    "tt = m.log_prob(x)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.7568])\n"
     ]
    }
   ],
   "source": [
    "mean = torch.tensor([[0., 0.,0.]])\n",
    "#mean = torch.tensor([[0.],[0.]])\n",
    "cov = torch.eye(3)\n",
    "x = torch.tensor([0., 0., 0.])\n",
    "m = MultivariateNormal(mean, cov)\n",
    "print(m.log_prob(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, 784)\n",
    "    # single hidden layer\n",
    "    def forward(self, x):\n",
    "        #x = x.view(-1, 784)\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return F.sigmoid(self.fc2(h1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_z0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(q_z0, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 300)\n",
    "        #self.fc2 = nn.Linear(300, 300)\n",
    "        self.fc31 = nn.Linear(300, z_dim)\n",
    "        self.fc32 = nn.Linear(300, z_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        #h2 = F.relu(self.fc2(h1))\n",
    "        logvar = self.fc31(h1)\n",
    "        mu = self.fc32(h1)\n",
    "        return mu, logvar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class r_v(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(r_v, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim + 784, 300)\n",
    "        self.fc21 = nn.Linear(300, z_dim)\n",
    "        self.fc22 = nn.Linear(300, z_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784 + z_dim)\n",
    "        h1 = F.softplus(self.fc1(x))\n",
    "        logvar = self.fc21(h1)\n",
    "        mu = self.fc22(h1)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_v(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(q_v, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 300)\n",
    "        # no need to output mu because the mean of momentum is default 0\n",
    "        self.fc21 = nn.Linear(300, z_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        h1 = F.softplus(self.fc1(x))\n",
    "        logvar = self.fc21(h1)\n",
    "        return logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9189, -0.9189])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.zeros(2,1)\n",
    "cov = torch.eye(2)\n",
    "m = MultivariateNormal(mean, cov)\n",
    "x = torch.tensor([0.,0.]) \n",
    "m.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoder().to(device)\n",
    "q_z0 = q_z0().to(device)\n",
    "r_v = r_v().to(device)\n",
    "q_v = q_v().to(device)\n",
    "log_mass_diag = torch.randn(z_dim, requires_grad=True)\n",
    "#mass = torch.randn(z_dim, requires_grad=True)\n",
    "#mass = torch.eye(z_dim, requires_grad=True)\n",
    "#mass_cuda = mass.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_bound(decoder, q_z0, r_v, data, log_mass_diag, T):\n",
    "    batch_size = data.view(-1, 784).shape[0]\n",
    "    data = data.to(device)\n",
    "    \n",
    "    #mu_z0 = q_z0_mean.repeat(batch_size,1).cuda()\n",
    "    #logvar_z0 = q_z0_logvar.repeat(batch_size,1).cuda()\n",
    "    \n",
    "    \n",
    "    mu_z0, logvar_z0 = q_z0(data)\n",
    "    var_z0 = torch.exp(logvar_z0)\n",
    "    #print(mu_z0.shape)\n",
    "    #print(logvar_z0.shape)\n",
    "\n",
    "    # sample z0\n",
    "    z0 = reparameterize(mu_z0, logvar_z0)\n",
    "    #print(\"z0: \" + str(z0.shape))\n",
    "    #print(z0)\n",
    "\n",
    "    # get joint probaility p(x, z0)\n",
    "    log_prior_z0 = log_prior(z0)\n",
    "    #print(\"log_prior_z0: \" + str(log_prior_z0.shape))\n",
    "    decoder_output = decoder(z0)\n",
    "    #print(\"decoder_output: \" + str(decoder_output.shape))\n",
    "    log_likelihood = 0. - F.binary_cross_entropy(decoder_output, data.view(-1, 784).float(), size_average=False, reduce=False)\n",
    "    #print(\"log_likelihood: \" + str(log_likelihood.shape))\n",
    "    log_likelihood = torch.sum(log_likelihood, dim = 1)\n",
    "    #print(\"log_likelihood: \" + str(log_likelihood.shape))\n",
    "    log_joint = log_prior_z0 + log_likelihood\n",
    "    #print(\"log_joint: \" + str(log_joint.shape))\n",
    "\n",
    "    # get log q_z0\n",
    "    log_q_z0 = multivariate_normal_diagonal_logpdf(mu_z0, var_z0, z0)\n",
    "\n",
    "    # initial L for 128 samples\n",
    "    L = log_joint - log_q_z0.view(batch_size)\n",
    "    L = torch.sum(L)\n",
    "    #print(\"L \"+str(L))\n",
    "    #print(L.shape)\n",
    "\n",
    "    #print(\"====================================\")\n",
    "    for i in range(T):\n",
    "        # sample v1\n",
    "        mass_diag = torch.exp(log_mass_diag)\n",
    "        mass_matrix = torch.diag(mass_diag)\n",
    "        mass_matrix.cuda()\n",
    "        \n",
    "        #var_v1_matrix = torch.inverse(mass_matrix)\n",
    "        #var_v1_diag = torch.diag(var_v1_matrix)\n",
    "        #logvar_v1_diag = torch.log(var_v1_diag)\n",
    "        \n",
    "        \n",
    "        logvar_v1_diag = -log_mass_diag\n",
    "        var_v1_diag = torch.exp(logvar_v1_diag)\n",
    "        \n",
    "        \n",
    "        \n",
    "        logvar_v1 = logvar_v1_diag.repeat(batch_size,1).cuda()\n",
    "        var_v1 = var_v1_diag.repeat(batch_size,1).cuda()\n",
    "        mu_v1 = torch.zeros(logvar_v1.shape[0], logvar_v1.shape[1]).cuda()\n",
    "        v1 = reparameterize(mu_v1, logvar_v1)\n",
    "        #print(v1)\n",
    "        \n",
    "        # get q_v1\n",
    "        \n",
    "        log_q_v1 = multivariate_normal_diagonal_logpdf(mu_v1, var_v1 ,v1)\n",
    "        \n",
    "\n",
    "        log_joint_t = torch.zeros(0).cuda() # list of all the joint\n",
    "        log_r_vt = torch.zeros(0).cuda()\n",
    "        alpha = torch.tensor([0.]).cuda() # lower bound for each batch (128 samples)\n",
    "        for j in range(batch_size):\n",
    "            def energy_function(z, cache):\n",
    "                z.retain_grad()\n",
    "                z = z.view(1, z.shape[0])\n",
    "                z = z.cuda()\n",
    "                one_log_prior = log_prior(z)\n",
    "                decoder_output = decoder(z)\n",
    "                one_log_likelihood = 0. - F.binary_cross_entropy(decoder_output, data.view(-1, 784)[j].float(), size_average=False, reduce=False)\n",
    "                #print(one_log_likelihood.shape)\n",
    "                one_log_likelihood = torch.sum(one_log_likelihood, dim = 1)\n",
    "                one_log_joint = one_log_prior + one_log_likelihood\n",
    "                return 0 - one_log_joint\n",
    "            sampler = IsotropicHmcSampler(energy_function, energy_grad=None, prng=None,\n",
    "                                          mom_resample_coeff=1., dtype=np.float64)\n",
    "            init = torch.zeros(z_dim).cuda()\n",
    "            \n",
    "            pos_samples, mom_samples, ratio = sampler.get_samples(init, 0.1, 3, 2, mass_matrix, mom = v1[j].view(z_dim))\n",
    "            #print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`\")\n",
    "            #print(pos_samples[1].shape)\n",
    "\n",
    "            # get joint probaility p(x, zt)\n",
    "            zt = pos_samples[1].cuda()\n",
    "            vt = mom_samples[1].cuda()\n",
    "            zt = zt.view(1, zt.shape[0])\n",
    "            vt = vt.view(vt.shape[0])\n",
    "\n",
    "            # get joint probaility p(x, zt)\n",
    "            one_log_prior_zt = log_prior(zt)\n",
    "            #print(\"one_log_prior_zt: \" + str(one_log_prior_zt.shape))\n",
    "            one_decoder_output_t = decoder(zt)\n",
    "            #print(\"one_decoder_output_t: \" + str(one_decoder_output_t.shape))\n",
    "            one_log_likelihood_t = 0. - F.binary_cross_entropy(one_decoder_output_t, data.view(-1, 784)[j].float(), size_average=False, reduce=False)\n",
    "            one_log_likelihood_t = torch.sum(one_log_likelihood_t, dim = 1)\n",
    "            #print(\"one_log_likelihood_t: \" + str(one_log_likelihood_t.shape))\n",
    "            one_log_joint_t = one_log_prior_zt + one_log_likelihood_t\n",
    "            #print(\"one_log_joint_t: \" + str(one_log_joint_t.shape))\n",
    "            log_joint_t = torch.cat((log_joint_t, one_log_joint_t), 0)\n",
    "\n",
    "            # get r_vt\n",
    "            d = data.view(-1, 784)[j].view(1, 784)\n",
    "            one_new_data = torch.cat((d.float(), zt), 1) # append data with zt\n",
    "            one_mu_vt, one_logvar_vt = r_v(one_new_data)\n",
    "            one_var_vt = torch.exp(one_logvar_vt)\n",
    "            one_mu_vt = one_mu_vt.view(one_mu_vt.shape[1])\n",
    "            one_cov = torch.diag(one_var_vt.view(one_var_vt.shape[1]))\n",
    "            #m = MultivariateNormal(one_mu_vt, one_cov)\n",
    "            #one_log_r_vt = m.log_prob(vt).view(1)\n",
    "            one_log_r_vt = multivariate_normal_logpdf(one_mu_vt, one_cov, vt)\n",
    "            log_r_vt = torch.cat((log_r_vt, one_log_r_vt), 0)\n",
    "            \n",
    "\n",
    "            # get L for each sample\n",
    "            one_log_alpha = log_joint_t[j] + log_r_vt[j] - log_joint[j] - log_q_v1[j]\n",
    "            #print(\"one log alpha: \"+str(one_log_alpha))\n",
    "            #one_log_alpha = torch.log(one_alpha)\n",
    "            L = L + one_log_alpha\n",
    "            #alpha = alpha + one_alpha\n",
    "        #L = L + torch.log(alpha)\n",
    "\n",
    "    #print(\"~~~~~~~~~~~~~~~~~~~ new L \" + str(L) + \" ~~~~~~~~~~~~~~~~~~~\")\n",
    "    return L/batch_size    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++ 0 ++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1474: UserWarning: Using a target size (torch.Size([784])) that is different to the input size (torch.Size([1, 784])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-547.0020751953125\n",
      "++++++++++ 1 ++++++++++\n",
      "-549.232177734375\n",
      "++++++++++ 2 ++++++++++\n",
      "-556.7857666015625\n",
      "++++++++++ 3 ++++++++++\n",
      "-559.0294799804688\n",
      "++++++++++ 4 ++++++++++\n",
      "-562.4439697265625\n",
      "++++++++++ 5 ++++++++++\n",
      "-573.060302734375\n",
      "++++++++++ 6 ++++++++++\n",
      "-581.3893432617188\n",
      "++++++++++ 7 ++++++++++\n",
      "-574.479248046875\n",
      "++++++++++ 8 ++++++++++\n",
      "-562.0333251953125\n",
      "++++++++++ 9 ++++++++++\n",
      "-564.1270141601562\n",
      "++++++++++ 10 ++++++++++\n",
      "-563.4755859375\n",
      "++++++++++ 11 ++++++++++\n",
      "-556.5250854492188\n",
      "++++++++++ 12 ++++++++++\n",
      "-543.0822143554688\n",
      "++++++++++ 13 ++++++++++\n",
      "-543.8929443359375\n",
      "++++++++++ 14 ++++++++++\n",
      "-532.5164794921875\n",
      "++++++++++ 15 ++++++++++\n",
      "-532.8471069335938\n",
      "++++++++++ 16 ++++++++++\n",
      "-533.5291137695312\n",
      "++++++++++ 17 ++++++++++\n",
      "-525.3306274414062\n",
      "++++++++++ 18 ++++++++++\n",
      "-517.4774169921875\n",
      "++++++++++ 19 ++++++++++\n",
      "-520.7122192382812\n",
      "++++++++++ 20 ++++++++++\n",
      "-509.89129638671875\n",
      "++++++++++ 21 ++++++++++\n",
      "-507.6878967285156\n",
      "++++++++++ 22 ++++++++++\n",
      "-507.7887878417969\n",
      "++++++++++ 23 ++++++++++\n",
      "-493.46795654296875\n",
      "++++++++++ 24 ++++++++++\n",
      "-489.7929382324219\n",
      "++++++++++ 25 ++++++++++\n",
      "-483.8854064941406\n",
      "++++++++++ 26 ++++++++++\n",
      "-481.03643798828125\n",
      "++++++++++ 27 ++++++++++\n",
      "-479.58843994140625\n",
      "++++++++++ 28 ++++++++++\n",
      "-475.1757507324219\n",
      "++++++++++ 29 ++++++++++\n",
      "-476.9544982910156\n",
      "++++++++++ 30 ++++++++++\n",
      "-472.91180419921875\n",
      "++++++++++ 31 ++++++++++\n",
      "-464.52325439453125\n",
      "++++++++++ 32 ++++++++++\n",
      "-467.07489013671875\n",
      "++++++++++ 33 ++++++++++\n",
      "-461.33416748046875\n",
      "++++++++++ 34 ++++++++++\n",
      "-453.8240661621094\n",
      "++++++++++ 35 ++++++++++\n",
      "-455.6805114746094\n",
      "++++++++++ 36 ++++++++++\n",
      "-443.3497314453125\n",
      "++++++++++ 37 ++++++++++\n",
      "-436.6253662109375\n",
      "++++++++++ 38 ++++++++++\n",
      "-434.20050048828125\n",
      "++++++++++ 39 ++++++++++\n",
      "-434.7978820800781\n",
      "++++++++++ 40 ++++++++++\n",
      "-431.8624572753906\n",
      "++++++++++ 41 ++++++++++\n",
      "-425.0139465332031\n",
      "++++++++++ 42 ++++++++++\n",
      "-437.3887634277344\n",
      "++++++++++ 43 ++++++++++\n",
      "-420.0441589355469\n",
      "++++++++++ 44 ++++++++++\n",
      "-406.0110778808594\n",
      "++++++++++ 45 ++++++++++\n",
      "-419.49298095703125\n",
      "++++++++++ 46 ++++++++++\n",
      "-402.6219177246094\n",
      "++++++++++ 47 ++++++++++\n",
      "-403.41204833984375\n",
      "++++++++++ 48 ++++++++++\n",
      "-405.1225280761719\n",
      "++++++++++ 49 ++++++++++\n",
      "-398.492919921875\n",
      "++++++++++ 50 ++++++++++\n",
      "-396.52288818359375\n",
      "++++++++++ 51 ++++++++++\n",
      "-391.7559814453125\n",
      "++++++++++ 52 ++++++++++\n",
      "-399.3083801269531\n",
      "++++++++++ 53 ++++++++++\n",
      "-388.9521484375\n",
      "++++++++++ 54 ++++++++++\n",
      "-379.1692199707031\n",
      "++++++++++ 55 ++++++++++\n",
      "-379.6139221191406\n",
      "++++++++++ 56 ++++++++++\n",
      "-379.1500549316406\n",
      "++++++++++ 57 ++++++++++\n",
      "-374.2121887207031\n",
      "++++++++++ 58 ++++++++++\n",
      "-369.4371032714844\n",
      "++++++++++ 59 ++++++++++\n",
      "-377.6238098144531\n",
      "++++++++++ 60 ++++++++++\n",
      "-376.1245422363281\n",
      "++++++++++ 61 ++++++++++\n",
      "-365.32275390625\n",
      "++++++++++ 62 ++++++++++\n",
      "-373.15875244140625\n",
      "++++++++++ 63 ++++++++++\n",
      "-363.49176025390625\n",
      "++++++++++ 64 ++++++++++\n",
      "-355.2478332519531\n",
      "++++++++++ 65 ++++++++++\n",
      "-349.25714111328125\n",
      "++++++++++ 66 ++++++++++\n",
      "-350.8782043457031\n",
      "++++++++++ 67 ++++++++++\n",
      "-356.24627685546875\n",
      "++++++++++ 68 ++++++++++\n",
      "-351.4630432128906\n",
      "++++++++++ 69 ++++++++++\n",
      "-342.1833801269531\n",
      "++++++++++ 70 ++++++++++\n",
      "-343.0956115722656\n",
      "++++++++++ 71 ++++++++++\n",
      "-346.66900634765625\n",
      "++++++++++ 72 ++++++++++\n",
      "-345.21826171875\n",
      "++++++++++ 73 ++++++++++\n",
      "-336.5341491699219\n",
      "++++++++++ 74 ++++++++++\n",
      "-338.2966613769531\n",
      "++++++++++ 75 ++++++++++\n",
      "inf\n",
      "++++++++++ 76 ++++++++++\n",
      "nan\n",
      "++++++++++ 77 ++++++++++\n",
      "nan\n",
      "++++++++++ 78 ++++++++++\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Lapack Error getrf : U(1,1) is 0, U is singular at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524546354046\\work\\aten\\src\\th\\generic/THTensorLapack.c:514",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-3aeba099b65a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0moptimizer2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0moptimizer3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_z0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_mass_diag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-ed4758bd4e71>\u001b[0m in \u001b[0;36mlower_bound\u001b[1;34m(decoder, q_z0, r_v, data, log_mass_diag, T)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mmass_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmass_diag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mmass_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mvar_v1_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmass_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mvar_v1_diag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_v1_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mlogvar_v1_diag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_v1_diag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Lapack Error getrf : U(1,1) is 0, U is singular at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524546354046\\work\\aten\\src\\th\\generic/THTensorLapack.c:514"
     ]
    }
   ],
   "source": [
    "params1 = list(decoder.parameters())+list(r_v.parameters())\n",
    "\n",
    "optimizer1 = optim.Adam(params1, lr=0.0001, weight_decay=1e-5)\n",
    "optimizer2 = optim.Adam([log_mass_diag], lr=0.0001)\n",
    "optimizer3 = optim.Adam(q_z0.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "nn.utils.clip_grad_norm_(q_z0.parameters(), 50)\n",
    "\n",
    "for i in range(len(result)):\n",
    "    print(\"++++++++++ \" + str(i) + \" ++++++++++\")\n",
    "    \n",
    "    data = result[i].float()\n",
    "    optimizer1.zero_grad()\n",
    "    optimizer2.zero_grad()\n",
    "    optimizer3.zero_grad()\n",
    "    L = lower_bound(decoder, q_z0, r_v, data, log_mass_diag, 1)\n",
    "    loss = 0. - L\n",
    "    loss.backward()\n",
    "    #print('weight grad after backward')\n",
    "    #print(net.conv1.bias.grad)\n",
    "    #print(q_z0.fc1.weight.grad)\n",
    "    #print(q_z0.fc31.weight.grad)\n",
    "    #print(q_z0.fc32.weight.grad)\n",
    "    optimizer1.step()\n",
    "    optimizer2.step()\n",
    "    optimizer3.step()\n",
    "    print(L.item())\n",
    "print(L.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++ 0 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1474: UserWarning: Using a target size (torch.Size([784])) that is different to the input size (torch.Size([1, 784])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-540.8672485351562\n",
      "++++++++++ 1 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-572.6387939453125\n",
      "++++++++++ 2 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-588.7979125976562\n",
      "++++++++++ 3 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-579.67236328125\n",
      "++++++++++ 4 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-559.671142578125\n",
      "++++++++++ 5 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-544.0812377929688\n",
      "++++++++++ 6 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-544.778076171875\n",
      "++++++++++ 7 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-516.0226440429688\n",
      "++++++++++ 8 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-505.8245544433594\n",
      "++++++++++ 9 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-497.9623107910156\n",
      "++++++++++ 10 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-487.72406005859375\n",
      "++++++++++ 11 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-463.9438781738281\n",
      "++++++++++ 12 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-466.0975341796875\n",
      "++++++++++ 13 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-452.3597717285156\n",
      "++++++++++ 14 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-450.99078369140625\n",
      "++++++++++ 15 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-429.5964050292969\n",
      "++++++++++ 16 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-434.8597412109375\n",
      "++++++++++ 17 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-417.3406677246094\n",
      "++++++++++ 18 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-407.9772033691406\n",
      "++++++++++ 19 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-411.21466064453125\n",
      "++++++++++ 20 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-403.7613525390625\n",
      "++++++++++ 21 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-404.00726318359375\n",
      "++++++++++ 22 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-399.34600830078125\n",
      "++++++++++ 23 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-390.32379150390625\n",
      "++++++++++ 24 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-378.6222229003906\n",
      "++++++++++ 25 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-375.39520263671875\n",
      "++++++++++ 26 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-372.48175048828125\n",
      "++++++++++ 27 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-361.93707275390625\n",
      "++++++++++ 28 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-359.9893493652344\n",
      "++++++++++ 29 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-373.8966979980469\n",
      "++++++++++ 30 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-359.6930236816406\n",
      "++++++++++ 31 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-350.148681640625\n",
      "++++++++++ 32 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-361.45147705078125\n",
      "++++++++++ 33 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-364.83154296875\n",
      "++++++++++ 34 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-356.56005859375\n",
      "++++++++++ 35 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-344.2277526855469\n",
      "++++++++++ 36 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-341.2362365722656\n",
      "++++++++++ 37 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-362.663330078125\n",
      "++++++++++ 38 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-345.8728332519531\n",
      "++++++++++ 39 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-363.71917724609375\n",
      "++++++++++ 40 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-352.9364318847656\n",
      "++++++++++ 41 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-353.8746337890625\n",
      "++++++++++ 42 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-342.87945556640625\n",
      "++++++++++ 43 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-326.8343811035156\n",
      "++++++++++ 44 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-360.9006042480469\n",
      "++++++++++ 45 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-344.5437927246094\n",
      "++++++++++ 46 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-338.8890686035156\n",
      "++++++++++ 47 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "-325.91522216796875\n",
      "++++++++++ 48 ++++++++++\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-219-1e33a8fd0192>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0moptimizer1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0moptimizer2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_z0_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_z0_logvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_mass_diag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-218-000e7370f612>\u001b[0m in \u001b[0;36mlower_bound\u001b[1;34m(decoder, q_z0_mean, q_z0_logvar, r_v, data, log_mass_diag, T)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mpos_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmom_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmass_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[1;31m#print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;31m#print(pos_samples[1].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Cambridge\\Project\\MHMC-for-VAE\\hmc_pytorch\\hmc_base_pytorch.py\u001b[0m in \u001b[0;36mget_samples\u001b[1;34m(self, pos, dt, n_step_per_sample, n_sample, mass, mom)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;31m# Metropolis-Hastings accept step on proposed update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             if (proposal_successful and self.prng.uniform() <\n\u001b[1;32m--> 114\u001b[1;33m                     torch.exp(hamiltonian_c - hamiltonian_p).item()):\n\u001b[0m\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# accept move\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mpos_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmom_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmom_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params1 = list(decoder.parameters())+list(q_z0.parameters())+list(r_v.parameters())\n",
    "optimizer1 = optim.Adam(params1, lr=0.0003)\n",
    "optimizer2 = optim.Adam([log_mass_diag, q_z0_mean, q_z0_logvar], lr=0.0003)\n",
    "nn.utils.clip_grad_norm_(q_z0.parameters(), 50)\n",
    "for batch_idx, (data, _) in enumerate(train_loader):\n",
    "    print(\"++++++++++ \" + str(batch_idx) + \" ++++++++++\")\n",
    "    \n",
    "    optimizer1.zero_grad()\n",
    "    optimizer2.zero_grad()\n",
    "    L = lower_bound(decoder, q_z0_mean, q_z0_logvar, r_v, data, log_mass_diag, 1)\n",
    "    loss = 0. - L\n",
    "    loss.backward()\n",
    "    #print('weight grad after backward')\n",
    "    #print(net.conv1.bias.grad)\n",
    "    #print(q_z0.fc1.weight.grad)\n",
    "    #print(q_z0.fc31.weight.grad)\n",
    "    #print(q_z0.fc32.weight.grad)\n",
    "    optimizer1.step()\n",
    "    optimizer2.step()\n",
    "    print(L.item())\n",
    "print(L.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "params argument given to the optimizer should be an iterable of Tensors or dicts, but got torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-143b7071f5f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#x = Variable(torch.randn(5))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\optim\\sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, lr, momentum, dampening, weight_decay, nesterov)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdampening\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Nesterov momentum requires a momentum and zero dampening\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m     29\u001b[0m             raise TypeError(\"params argument given to the optimizer should be \"\n\u001b[0;32m     30\u001b[0m                             \u001b[1;34m\"an iterable of Tensors or dicts, but got \"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                             torch.typename(params))\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: params argument given to the optimizer should be an iterable of Tensors or dicts, but got torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "#params = mass_diag\n",
    "\n",
    "\n",
    "#optimizer = optim.Adam(params, lr=1e-3)\n",
    "\n",
    "w = torch.randn([3,5], requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "#x = Variable(torch.randn(5))\n",
    "\n",
    "optimizer = optim.SGD([w], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 20])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(decoder.parameters())\n",
    "params[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(decoder.parameters())[0]\n",
    "#list(mass_diag)[0]\n",
    "mass_diag.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = torch.ones(1, 784).cuda()\n",
    "print(tt.shape)\n",
    "torch.sum(tt, dim=1)\n",
    "tt.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0.1, 0.1],[0.1,0.1],[0.1,0.1]])\n",
    "y = torch.tensor([[0.5, 0.5],[0.5,0.5],[1.,1.]])\n",
    "L = F.binary_cross_entropy(y, x, size_average=False, reduce=False)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Lapack Error getrf : U(2,2) is 0, U is singular at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524546354046\\work\\aten\\src\\th\\generic/THTensorLapack.c:514",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-dee977365fee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Lapack Error getrf : U(2,2) is 0, U is singular at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524546354046\\work\\aten\\src\\th\\generic/THTensorLapack.c:514"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0.1, 0.1],[0.1,0.1]])\n",
    "torch.inverse(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
